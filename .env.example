# Qdrant connection
QDRANT_URL=http://localhost:6333
# QDRANT_API_KEY=

# Multi-repo mode: 0=single-repo (default), 1=multi-repo
# Single-repo: All files go into one collection (COLLECTION_NAME)
# Multi-repo: Each subdirectory gets its own collection
MULTI_REPO_MODE=0

# Logical repo reuse (experimental): 0=disabled (default), 1=enable logical_repo_id-based
# collection reuse across git worktrees / clones. When enabled, indexer, watcher, and
# upload service will try to reuse a canonical collection per logical repository and
# use (repo_id + repo_rel_path) for skip-unchanged across worktrees.
#LOGICAL_REPO_REUSE=0

# Single unified collection for seamless cross-repo search (default: "codebase")
# Leave unset or use "codebase" for unified search across all your code
COLLECTION_NAME=codebase


# Embeddings
EMBEDDING_MODEL=BAAI/bge-base-en-v1.5
EMBEDDING_PROVIDER=fastembed
# Optional repo tag attached to each payload
REPO_NAME=workspace

# Cross-Codebase Isolation (multi-repo search scoping)
# When enabled, search results are automatically filtered to the current repo
# Disable (=0) to search all repos by default (legacy behavior)
REPO_AUTO_FILTER=1
# Explicitly set current repo (overrides auto-detection from git/directory)
# CURRENT_REPO=my-project

# MCP servers (SSE)
FASTMCP_HOST=0.0.0.0
FASTMCP_PORT=8000            # search/store MCP (mcp-server-qdrant)
FASTMCP_INDEXER_PORT=8001    # companion indexer MCP (index/prune/list)
FASTMCP_SERVER_NAME=qdrant-mcp
# MCP_MAX_LOG_TAIL=4000      # Max chars for subprocess stdout/stderr tail (default: 4000)

# Transport: sse (default), http (streamable), or stdio
FASTMCP_TRANSPORT=sse


# Optional duplicate Streamable HTTP instances (run alongside SSE)
# Use these in docker-compose overrides to expose /mcp/ on separate ports.
FASTMCP_HTTP_TRANSPORT=http
FASTMCP_HTTP_PORT=8002
FASTMCP_HTTP_HEALTH_PORT=18002
FASTMCP_INDEXER_HTTP_PORT=8003
FASTMCP_INDEXER_HTTP_HEALTH_PORT=18003


# Optional: local cross-encoder reranker (ONNX)
# Set these to enable make rerank-local inside the container
RERANKER_ONNX_PATH=/work/models/model_qint8_avx512_vnni.onnx
RERANKER_TOKENIZER_PATH=/work/models/tokenizer.json

# Enable reranking in the indexer MCP search path
RERANKER_ENABLED=0
# Tuning knobs (effective when enabled)
RERANKER_TOPN=50
RERANKER_RETURN_M=12
RERANKER_TIMEOUT_MS=2000

# Post-rerank symbol boost: ensures exact symbol matches rank highest even when
# the neural reranker disagrees. Applied after rerank blending as a direct score addition.
# Set to 0 to disable, >1.0 to boost symbol matches more aggressively.
POST_RERANK_SYMBOL_BOOST=1.0
# Rerank blend weight: ratio of rerank score vs fusion score (0.0-1.0)
# Higher = more weight on neural reranker, lower = more weight on lexical/symbol boosts
RERANK_BLEND_WEIGHT=0.6

# Safety: minimum rerank timeout floor (ms) to avoid cold-start timeouts
RERANK_TIMEOUT_FLOOR_MS=1000

# Optional warmups (disabled by default)
EMBEDDING_WARMUP=0
RERANK_WARMUP=0


# In-process execution (faster; falls back to subprocess on failure)
HYBRID_IN_PROCESS=1
RERANK_IN_PROCESS=1

# LLM query expansion (prefer local runtime)
# If OLLAMA_HOST is reachable, we use it by default with LLM_EXPAND_MODEL (e.g., glm4 or glm-4.5-air)
# Otherwise, if OPENAI_API_KEY is set, we fallback to OpenAI with the same model name
# Local LLM expansion via Ollama (mini model)
LLM_PROVIDER=ollama
OLLAMA_HOST=http://host.docker.internal:11434
LLM_EXPAND_MODEL=phi3:mini
LLM_EXPAND_MAX=4
# PRF defaults (enabled by default)
PRF_ENABLED=1

# Tree-sitter parsing (enable for more accurate symbols/scopes)

# Indexer scaling and exclusions
# Exclusions: defaults can be disabled or extended
# QDRANT_DEFAULT_EXCLUDES=0
# QDRANT_IGNORE_FILE=.qdrantignore
# QDRANT_EXCLUDES=tokenizer.json,*.onnx,/vendor
# Chunking + batching (tune for large repos)
# INDEX_CHUNK_LINES=120
# INDEX_CHUNK_OVERLAP=20
# INDEX_BATCH_SIZE=64
# INDEX_PROGRESS_EVERY=200

USE_TREE_SITTER=0


# ReFRAG mode (optional): compact gating + micro-chunking
# Enable to add a 64-dim mini vector for fast gating and use token-based micro-chunks
REFRAG_MODE=0
MINI_VECTOR_NAME=mini
MINI_VEC_DIM=64
MINI_VEC_SEED=1337
HYBRID_MINI_WEIGHT=0.5
# Micro-chunking controls (token-based)
INDEX_MICRO_CHUNKS=0
MICRO_CHUNK_TOKENS=16
MICRO_CHUNK_STRIDE=8
# Optional: gate-first using mini vectors to prefilter dense search
REFRAG_GATE_FIRST=0
REFRAG_CANDIDATES=200

# Output shaping for micro spans (defaults shown)
MICRO_OUT_MAX_SPANS=3
MICRO_MERGE_LINES=4
MICRO_BUDGET_TOKENS=512
MICRO_TOKENS_PER_LINE=32

# Decoder-path ReFRAG (feature-flagged; off by default)
REFRAG_DECODER=0
REFRAG_RUNTIME=llamacpp
REFRAG_ENCODER_MODEL=BAAI/bge-base-en-v1.5
REFRAG_PHI_PATH=/work/models/refrag_phi_768_to_dmodel.json
REFRAG_SENSE=heuristic

# Enable index-time pseudo descriptions for micro-chunks (requires REFRAG_DECODER)
# REFRAG_PSEUDO_DESCRIBE=1

# Llama.cpp sidecar (optional)
# Docker CPU-only (stable): http://llamacpp:8080
# Native GPU-accelerated (fast): http://localhost:8081
LLAMACPP_URL=http://llamacpp:8080
REFRAG_DECODER_MODE=prompt  # prompt|soft

# GLM_API_BASE=https://api.z.ai/api/coding/paas/v4/
# GLM_MODEL=glm-4.6

# GPU Performance Toggle
# Set to 1 to use native GPU-accelerated server on localhost:8081
# Set to 0 to use Docker CPU-only server (default, stable)
USE_GPU_DECODER=0

REFRAG_SOFT_SCALE=1.0

# Llama.cpp runtime tuning
LLAMACPP_USE_GPU=0           # Set to 1 to enable Metal/CLBlast acceleration
# LLAMACPP_GPU_LAYERS=-1     # Override number of layers to offload (defaults to -1 when USE_GPU=1)
# LLAMACPP_GPU_SPLIT=         # Optional tensor split for multi-GPU setups
# LLAMACPP_THREADS=           # Override number of CPU threads
# LLAMACPP_CTX_SIZE=8192      # Context tokens; higher values need more VRAM
# LLAMACPP_EXTRA_ARGS=        # Additional flags passed verbatim to llama.cpp

# Operational safeguards and timeouts
# Limit explosion of micro-chunks on huge files (0 to disable)
MAX_MICRO_CHUNKS_PER_FILE=2000
# Qdrant request timeout (seconds)
QDRANT_TIMEOUT=20
# Memory collection auto-detection cache
MEMORY_AUTODETECT=1
MEMORY_COLLECTION_TTL_SECS=300

# Smarter re-indexing for symbol cache, reuse embeddings and reduce decoder/pseudo tags to re-index
SMART_SYMBOL_REINDEXING=0

# Watcher-safe defaults (recommended)
# Applied to watcher via compose; uncomment to apply globally.
# Qdrant write/read timeout (seconds)
# QDRANT_TIMEOUT=60
# Cap per-file micro-chunks (keeps upserts small)
# MAX_MICRO_CHUNKS_PER_FILE=200
# Upsert batching + retries for large payloads
# INDEX_UPSERT_BATCH=128
# INDEX_UPSERT_RETRIES=5
# INDEX_UPSERT_BACKOFF=0.5
# Debounce file events to coalesce bursts
# WATCH_DEBOUNCE_SECS=1.5

# Remote upload git history (used by upload clients)
# Max number of commits to include per bundle (0 disables git history)
# REMOTE_UPLOAD_GIT_MAX_COMMITS=500
# Optional git log since filter, e.g. '6 months ago' or '2024-01-01'
# REMOTE_UPLOAD_GIT_SINCE=
# Enable commit lineage goals for indexing
REFRAG_COMMIT_DESCRIBE=1

STRICT_MEMORY_RESTORE=0

# info_request() tool settings (simplified codebase retrieval)
# Default result limit for info_request queries
INFO_REQUEST_LIMIT=10
# Default context lines in snippets (richer than repo_search default)
INFO_REQUEST_CONTEXT_LINES=5
# Enable explanation mode by default (summary, primary_locations, related_concepts)
# INFO_REQUEST_EXPLAIN_DEFAULT=0
# Enable relationship mapping by default (imports_from, calls, related_paths)
# INFO_REQUEST_RELATIONSHIPS=0
