# Environment for local Qdrant + MCP server (SSE)
# Qdrant (vector DB) runs as docker service "qdrant". The MCP server connects to it internally.
QDRANT_URL=http://qdrant:6333
# QDRANT_API_KEY= # not needed for local

# Default collection used by the MCP server (auto-created if missing)
COLLECTION_NAME=my-collection

# Embedding settings (FastEmbed model)
EMBEDDING_MODEL=BAAI/bge-base-en-v1.5
EMBEDDING_PROVIDER=fastembed

# FastMCP server settings (SSE transport)
FASTMCP_HOST=0.0.0.0
FASTMCP_PORT=8000
FASTMCP_INDEXER_PORT=8001

# Optional: customize tool descriptions (uncomment to use)
TOOL_STORE_DESCRIPTION="Store reusable code snippets for later retrieval. The 'information' is a clear NL description; include the actual code in 'metadata.code' and add 'metadata.language' (e.g., python, typescript) and 'metadata.path' when known. Use this whenever you generate or refine a code snippet."
TOOL_FIND_DESCRIPTION="Search for relevant code snippets using multiple phrasings of the query (multi-query). Prefer results where metadata.language matches the target file and metadata.path is relevant. You may pass optional filters (language, path_prefix, kind) which the server applies server-side. Include 'metadata.code', 'metadata.path', and 'metadata.language' in responses."



# Optional: local cross-encoder reranker (ONNX)
# Set these to enable make rerank-local inside the container
RERANKER_ONNX_PATH=/work/models/model_qint8_avx512_vnni.onnx
RERANKER_TOKENIZER_PATH=/work/models/tokenizer.json

# Reranker toggles and tuning
RERANKER_ENABLED=1
RERANKER_TOPN=50
RERANKER_RETURN_M=12
RERANKER_TIMEOUT_MS=2000

# Safety: minimum rerank timeout floor (ms) to avoid cold-start timeouts
RERANK_TIMEOUT_FLOOR_MS=1000

# Optional warmups (disabled by default)
EMBEDDING_WARMUP=0
RERANK_WARMUP=0


# In-process execution (faster; falls back to subprocess on failure)
HYBRID_IN_PROCESS=1
RERANK_IN_PROCESS=1


# Tree-sitter parsing (enable for more accurate symbols/scopes)
USE_TREE_SITTER=1


# Hybrid/rerank quick-win defaults (can override via flags)
HYBRID_EXPAND=1
HYBRID_PER_PATH=1
HYBRID_SYMBOL_BOOST=0.15
HYBRID_RECENCY_WEIGHT=0.1
RERANK_EXPAND=1

INDEX_SEMANTIC_CHUNKS=1


# Memory integration (SSE + Qdrant)
MEMORY_SSE_ENABLED=true
MEMORY_MCP_URL=http://mcp:8000/sse
MEMORY_MCP_TIMEOUT=6


# Local LLM expansion via Ollama (mini model)
LLM_PROVIDER=ollama
OLLAMA_HOST=http://host.docker.internal:11434
LLM_EXPAND_MODEL=phi3:mini
LLM_EXPAND_MAX=4
# PRF defaults (enabled by default)
PRF_ENABLED=1


# ReFRAG mode: compact gating + micro-chunking
REFRAG_MODE=1
MINI_VECTOR_NAME=mini
MINI_VEC_DIM=64
MINI_VEC_SEED=1337
HYBRID_MINI_WEIGHT=1.0
# Micro-chunking controls (token-based)
INDEX_MICRO_CHUNKS=1
MICRO_CHUNK_TOKENS=16
MICRO_CHUNK_STRIDE=8
# Optional: gate-first using mini vectors to prefilter dense search
REFRAG_GATE_FIRST=0
REFRAG_CANDIDATES=200

# Output shaping for micro spans
MICRO_OUT_MAX_SPANS=3
MICRO_MERGE_LINES=4
MICRO_BUDGET_TOKENS=512
MICRO_TOKENS_PER_LINE=32

# Decoder-path ReFRAG (feature-flagged; off by default)
REFRAG_DECODER=0
REFRAG_RUNTIME=llamacpp
REFRAG_ENCODER_MODEL=BAAI/bge-base-en-v1.5
REFRAG_PHI_PATH=/work/models/refrag_phi_768_to_dmodel.bin
REFRAG_SENSE=heuristic

# Llama.cpp sidecar (optional)
LLAMACPP_URL=http://llamacpp:8080
REFRAG_DECODER_MODE=prompt  # prompt|soft

REFRAG_SOFT_SCALE=1.0


# Operational safeguards and timeouts
MAX_MICRO_CHUNKS_PER_FILE=200
QDRANT_TIMEOUT=20
MEMORY_AUTODETECT=1
MEMORY_COLLECTION_TTL_SECS=300


# Watcher-safe defaults (recommended)
# These are applied to the watcher service via docker-compose.yml.
# Uncomment to apply globally if you are not using compose overrides.
# QDRANT_TIMEOUT=60
# MAX_MICRO_CHUNKS_PER_FILE=200
# INDEX_UPSERT_BATCH=128
# INDEX_UPSERT_RETRIES=5
# INDEX_UPSERT_BACKOFF=0.5
# WATCH_DEBOUNCE_SECS=1.5
