apiVersion: v1
kind: ConfigMap
metadata:
  name: context-engine-config
  namespace: context-engine
  labels:
    app: context-engine
    component: configuration
data:
  COLLECTION_NAME: codebase
  CTX_SNIPPET_CHARS: '400'
  CTX_SUMMARY_CHARS: '0'
  DECODER_MAX_TOKENS: '4000'
  EMBEDDING_MODEL: BAAI/bge-base-en-v1.5
  EMBEDDING_PROVIDER: fastembed
  EMBEDDING_WARMUP: '0'
  FASTMCP_HOST: 0.0.0.0
  FASTMCP_HTTP_HEALTH_PORT: '18002'
  FASTMCP_HTTP_PORT: '8002'
  FASTMCP_HTTP_TRANSPORT: http
  FASTMCP_INDEXER_HTTP_HEALTH_PORT: '18003'
  FASTMCP_INDEXER_HTTP_PORT: '8003'
  FASTMCP_INDEXER_PORT: '8001'
  FASTMCP_PORT: '8000'
  GLM_API_BASE: https://api.z.ai/api/coding/paas/v4/
  GLM_MODEL: glm-4.6
  HOST_INDEX_PATH: ./dev-workspace
  HYBRID_EXPAND: '0'
  HYBRID_IN_PROCESS: '1'
  HYBRID_MINI_WEIGHT: '1.0'
  HYBRID_PER_PATH: '1'
  HYBRID_RECENCY_WEIGHT: '0.1'
  HYBRID_RESULTS_CACHE: '128'
  HYBRID_RESULTS_CACHE_ENABLED: '1'
  HYBRID_SYMBOL_BOOST: '0.35'
  INDEX_CHUNK_LINES: '60'
  INDEX_CHUNK_OVERLAP: '10'
  INDEX_MICRO_CHUNKS: '0'
  INDEX_SEMANTIC_CHUNKS: '1'
  INDEX_UPSERT_BACKOFF: '0.5'
  INDEX_UPSERT_BATCH: '128'
  INDEX_UPSERT_RETRIES: '5'
  LLAMACPP_EXTRA_ARGS: ''
  LLAMACPP_GPU_LAYERS: '32'
  LLAMACPP_GPU_SPLIT: ''
  LLAMACPP_THREADS: '6'
  LLAMACPP_TIMEOUT_SEC: '300'
  LLAMACPP_URL: http://host.docker.internal:8081
  LLAMACPP_USE_GPU: '1'
  LLM_EXPAND_MAX: '0'
  LLM_EXPAND_MODEL: phi3:mini
  LLM_PROVIDER: ollama
  MAX_CHANGED_SYMBOLS_RATIO: '0.6'
  MAX_EMBED_CACHE: '16384'
  MAX_MICRO_CHUNKS_PER_FILE: '500'
  MCP_INDEXER_URL: http://localhost:8003/mcp
  MEMORY_AUTODETECT: '1'
  MEMORY_COLLECTION_TTL_SECS: '300'
  MEMORY_MCP_TIMEOUT: '6'
  MEMORY_MCP_URL: http://mcp:8000/sse
  MEMORY_SSE_ENABLED: 'true'
  MICRO_BUDGET_TOKENS: '1500'
  MICRO_CHUNK_STRIDE: '48'
  MICRO_CHUNK_TOKENS: '24'
  MICRO_MERGE_LINES: '4'
  MICRO_OUT_MAX_SPANS: '3'
  MICRO_TOKENS_PER_LINE: '32'
  MINI_VECTOR_NAME: mini
  MINI_VEC_DIM: '64'
  MINI_VEC_SEED: '1337'
  MULTI_REPO_MODE: '1'
  OLLAMA_HOST: http://host.docker.internal:11434
  PRF_ENABLED: '1'
  QDRANT_API_KEY: ''
  QDRANT_TIMEOUT: '20'
  QDRANT_URL: http://qdrant:6333
  REFRAG_CANDIDATES: '200'
  REFRAG_COMMIT_DESCRIBE: '1'
  REFRAG_DECODER: '1'
  REFRAG_DECODER_MODE: prompt
  REFRAG_ENCODER_MODEL: BAAI/bge-base-en-v1.5
  REFRAG_GATE_FIRST: '1'
  REFRAG_MODE: '1'
  REFRAG_PHI_PATH: /work/models/refrag_phi_768_to_dmodel.bin
  REFRAG_PSEUDO_DESCRIBE: '1'
  REFRAG_RUNTIME: glm
  REFRAG_SENSE: heuristic
  REFRAG_SOFT_SCALE: '1.0'
  REMOTE_UPLOAD_GIT_MAX_COMMITS: '500'
  RERANKER_ENABLED: '1'
  RERANKER_ONNX_PATH: /work/models/model_qint8_avx512_vnni.onnx
  RERANKER_RETURN_M: '20'
  RERANKER_TIMEOUT_MS: '3000'
  RERANKER_TOKENIZER_PATH: /work/models/tokenizer.json
  RERANKER_TOPN: '100'
  RERANK_EXPAND: '1'
  RERANK_IN_PROCESS: '1'
  RERANK_TIMEOUT_FLOOR_MS: '1000'
  RERANK_WARMUP: '0'
  SMART_SYMBOL_REINDEXING: '1'
  STRICT_MEMORY_RESTORE: '1'
  TOOL_FIND_DESCRIPTION: Search for relevant code snippets using multiple phrasings
    of the query (multi-query). Prefer results where metadata.language matches the
    target file and metadata.path is relevant. You may pass optional filters (language,
    path_prefix, kind) which the server applies server-side. Include 'metadata.code',
    'metadata.path', and 'metadata.language' in responses.
  TOOL_STORE_DESCRIPTION: Store reusable code snippets for later retrieval. The 'information'
    is a clear NL description; include the actual code in 'metadata.code' and add
    'metadata.language' (e.g., python, typescript) and 'metadata.path' when known.
    Use this whenever you generate or refine a code snippet.
  USE_GPU_DECODER: '0'
  USE_TREE_SITTER: '1'
  WATCH_DEBOUNCE_SECS: '4'
