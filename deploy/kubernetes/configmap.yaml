apiVersion: v1
kind: ConfigMap
metadata:
  name: context-engine-config
  namespace: context-engine
  labels:
    app: context-engine
    component: configuration
data:
  COLLECTION_NAME: "codebase"
  EMBEDDING_MODEL: "BAAI/bge-base-en-v1.5"
  EMBEDDING_PROVIDER: "fastembed"

  FASTMCP_HOST: "0.0.0.0"
  FASTMCP_PORT: "8000"
  FASTMCP_INDEXER_PORT: "8001"

  TOOL_STORE_DESCRIPTION: "Store reusable code snippets for later retrieval. The 'information' is a clear NL description; include the actual code in 'metadata.code' and add 'metadata.language' (e.g., python, typescript) and 'metadata.path' when known. Use this whenever you generate or refine a code snippet."
  TOOL_FIND_DESCRIPTION: "Search for relevant code snippets using multiple phrasings of the query (multi-query). Prefer results where metadata.language matches the target file and metadata.path is relevant. You may pass optional filters (language, path_prefix, kind) which the server applies server-side. Include 'metadata.code', 'metadata.path', and 'metadata.language' in responses."

  RERANKER_ENABLED: "1"
  RERANKER_TOPN: "100"
  RERANKER_RETURN_M: "20"
  RERANKER_TIMEOUT_MS: "3000"
  RERANK_TIMEOUT_FLOOR_MS: "1000"

  EMBEDDING_WARMUP: "0"
  RERANK_WARMUP: "0"

  HYBRID_IN_PROCESS: "1"
  RERANK_IN_PROCESS: "1"

  USE_TREE_SITTER: "1"

  HYBRID_EXPAND: "1"
  HYBRID_PER_PATH: "1"
  HYBRID_SYMBOL_BOOST: "0.35"
  HYBRID_RECENCY_WEIGHT: "0.1"
  RERANK_EXPAND: "1"

  INDEX_SEMANTIC_CHUNKS: "0"

  MEMORY_SSE_ENABLED: "true"
  MEMORY_MCP_URL: "http://mcp:8000/sse"
  MEMORY_MCP_TIMEOUT: "6"

  LLM_PROVIDER: "ollama"
  OLLAMA_HOST: "http://ollama:11434"
  LLM_EXPAND_MODEL: "phi3:mini"
  LLM_EXPAND_MAX: "4"
  PRF_ENABLED: "1"

  REFRAG_MODE: "1"
  MINI_VECTOR_NAME: "mini"
  MINI_VEC_DIM: "64"
  MINI_VEC_SEED: "1337"
  HYBRID_MINI_WEIGHT: "1.0"

  INDEX_MICRO_CHUNKS: "1"
  MICRO_CHUNK_TOKENS: "16"
  MICRO_CHUNK_STRIDE: "8"
  REFRAG_GATE_FIRST: "1"
  REFRAG_CANDIDATES: "200"

  MICRO_OUT_MAX_SPANS: "3"
  MICRO_MERGE_LINES: "4"
  MICRO_BUDGET_TOKENS: "512"
  MICRO_TOKENS_PER_LINE: "32"

  CTX_SUMMARY_CHARS: "0"

  REFRAG_DECODER: "1"
  REFRAG_RUNTIME: "llamacpp"
  REFRAG_ENCODER_MODEL: "BAAI/bge-base-en-v1.5"
  REFRAG_PHI_PATH: "/work/models/refrag_phi_768_to_dmodel.bin"
  REFRAG_SENSE: "heuristic"

  LLAMACPP_URL: "http://llamacpp:8080"
  LLAMACPP_TIMEOUT_SEC: "180"
  DECODER_MAX_TOKENS: "4000"
  REFRAG_DECODER_MODE: "prompt"
  REFRAG_SOFT_SCALE: "1.0"

  MAX_MICRO_CHUNKS_PER_FILE: "200"
  QDRANT_TIMEOUT: "60"
  MEMORY_AUTODETECT: "1"
  MEMORY_COLLECTION_TTL_SECS: "300"

  FASTMCP_HTTP_TRANSPORT: "http"
  FASTMCP_HTTP_PORT: "8002"
  FASTMCP_HTTP_HEALTH_PORT: "18002"
  FASTMCP_INDEXER_HTTP_PORT: "8003"
  FASTMCP_INDEXER_HTTP_HEALTH_PORT: "18003"

  WATCH_DEBOUNCE_SECS: "1.5"
  INDEX_UPSERT_BATCH: "128"
  INDEX_UPSERT_RETRIES: "5"

  QDRANT_URL: "http://qdrant:6333"

  QDRANT_API_KEY: ""
  REPO_NAME: "workspace"
  FASTMCP_SERVER_NAME: "qdrant-mcp"
  HOST_INDEX_PATH: "/work"

  INDEX_CHUNK_LINES: "120"
  INDEX_CHUNK_OVERLAP: "20"
  INDEX_BATCH_SIZE: "64"
  INDEX_UPSERT_BACKOFF: "0.5"
  FASTMCP_HEALTH_PORT: "18000"
  CTX_MULTI_COLLECTION: "1"
  CTX_DOC_PASS: "1"
  DEBUG_CONTEXT_ANSWER: "0"
  TOKENIZER_JSON: "/app/models/tokenizer.json"
  LLAMACPP_MODEL_URL: "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q8_0.gguf"
  LLAMACPP_MODEL_NAME: "qwen2.5-1.5b-instruct-q8_0.gguf"
