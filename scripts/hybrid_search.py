#!/usr/bin/env python3
"""
hybrid_search.py - Façade module for hybrid code search.

This is the stable public entrypoint for the hybrid search subsystem.
All internal logic has been refactored into smaller, focused modules:
- hybrid_config.py: Environment-based configuration and constants
- hybrid_qdrant.py: Qdrant client management, queries, and vector functions
- hybrid_embed.py: Embedding model factory and cached embedding
- hybrid_filters.py: File classification and query DSL parsing
- hybrid_ranking.py: RRF, scoring, diversification, and micro-span budgeting
- hybrid_expand.py: Query expansion (synonyms, semantic, LLM-assisted)

This façade:
1. Re-exports all public APIs for backwards compatibility
2. Implements run_hybrid_search (the main search orchestrator)
3. Provides the CLI entrypoint (main)
"""
from __future__ import annotations

import os
import sys
import argparse
import re
import json
import math
import logging
import threading
from pathlib import Path
from typing import List, Dict, Any, Tuple, TYPE_CHECKING
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor

# Ensure /work or repo root is in sys.path for scripts imports
_ROOT_DIR = Path(__file__).resolve().parent.parent
if str(_ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(_ROOT_DIR))

# ---------------------------------------------------------------------------
# Core Qdrant imports
# ---------------------------------------------------------------------------
from qdrant_client import QdrantClient, models

# ---------------------------------------------------------------------------
# Re-exports from hybrid_config
# ---------------------------------------------------------------------------
from scripts.hybrid_config import (
    # Helper functions
    _safe_int,
    _safe_float,
    _env_truthy,
    # Core constants
    MODEL_NAME,
    QDRANT_URL,
    API_KEY,
    # Lexical vector config
    LEX_VECTOR_NAME,
    LEX_VECTOR_DIM,
    LEX_SPARSE_NAME,
    LEX_SPARSE_MODE,
    # Mini vector config
    MINI_VECTOR_NAME,
    MINI_VEC_DIM,
    HYBRID_MINI_WEIGHT,
    # RRF and scoring constants
    RRF_K,
    DENSE_WEIGHT,
    LEXICAL_WEIGHT,
    LEX_VECTOR_WEIGHT,
    EF_SEARCH,
    SYMBOL_BOOST,
    SYMBOL_EQUALITY_BOOST,
    FNAME_BOOST,
    RECENCY_WEIGHT,
    CORE_FILE_BOOST,
    VENDOR_PENALTY,
    LANG_MATCH_BOOST,
    CLUSTER_LINES,
    TEST_FILE_PENALTY,
    CONFIG_FILE_PENALTY,
    IMPLEMENTATION_BOOST,
    DOCUMENTATION_PENALTY,
    PSEUDO_BOOST,
    COMMENT_PENALTY,
    COMMENT_RATIO_THRESHOLD,
    INTENT_IMPL_BOOST,
    # Sparse lexical scoring
    SPARSE_LEX_MAX_SCORE,
    SPARSE_RRF_MAX,
    SPARSE_RRF_MIN,
    # Micro-span budgeting
    MICRO_OUT_MAX_SPANS,
    MICRO_MERGE_LINES,
    MICRO_BUDGET_TOKENS,
    MICRO_TOKENS_PER_LINE,
    # Large collection scaling
    LARGE_COLLECTION_THRESHOLD,
    MAX_RRF_K_SCALE,
    SCORE_NORMALIZE_ENABLED,
    # Collection resolution
    _collection,
    # Cache config
    MAX_EMBED_CACHE,
    MAX_RESULTS_CACHE,
    # Output config
    INCLUDE_WHY,
)

# ---------------------------------------------------------------------------
# Re-exports from hybrid_qdrant
# ---------------------------------------------------------------------------
from scripts.hybrid_qdrant import (
    # Pool availability
    _POOL_AVAILABLE,
    # Connection pooling
    get_qdrant_client,
    return_qdrant_client,
    pooled_qdrant_client,
    # Thread executor
    _QUERY_EXECUTOR,
    _EXECUTOR_LOCK,
    _get_query_executor,
    # Point coercion
    _coerce_points,
    # Legacy search
    _legacy_vector_search,
    # Collection caching
    _ENSURED_COLLECTIONS,
    _get_client_endpoint,
    _ensure_collection,
    # Lexical vector functions
    lex_hash_vector,
    lex_sparse_vector,
    # Query functions
    lex_query,
    sparse_lex_query,
    dense_query,
    # Filter sanitization
    _sanitize_filter_obj,
)

# ---------------------------------------------------------------------------
# Re-exports from hybrid_embed
# ---------------------------------------------------------------------------
from scripts.hybrid_embed import (
    # Embedder factory
    _EMBEDDER_FACTORY,
    EmbeddingModel,
    get_embedding_model,
    # Cached embedding
    embed_queries_cached,
    _embed_queries_cached,  # Legacy alias
    clear_embedding_cache,
    get_embedding_cache_stats,
    # Constants
    UNIFIED_CACHE_AVAILABLE,
)

# Import unified cache objects from cache_manager when available
if UNIFIED_CACHE_AVAILABLE:
    try:
        from scripts.cache_manager import get_search_cache, get_embedding_cache, get_expansion_cache
        _EMBED_CACHE = get_embedding_cache()
        _RESULTS_CACHE = get_search_cache()
        _EXPANSION_CACHE = get_expansion_cache()
    except ImportError:
        _EMBED_CACHE = None
        _RESULTS_CACHE = {}
        _EXPANSION_CACHE = None
else:
    _EMBED_CACHE = None
    _RESULTS_CACHE = {}
    _EXPANSION_CACHE = None

# Lightweight local fallback cache for deterministic test hits
try:
    from collections import OrderedDict as _OD
except Exception:
    _OD = dict  # pragma: no cover
_RESULTS_CACHE_OD = _OD()
_RESULTS_LOCK = threading.RLock()

# ---------------------------------------------------------------------------
# Re-exports from hybrid_filters
# ---------------------------------------------------------------------------
from scripts.hybrid_filters import (
    # File patterns
    CORE_FILE_PATTERNS,
    NON_CORE_PATTERNS,
    TEST_FILE_PATTERNS,
    VENDOR_PATTERNS,
    LANG_EXTS,
    # Classification functions
    is_test_file,
    is_core_file,
    is_vendor_path,
    lang_matches_path,
    # Query DSL
    parse_query_dsl,
    # Tokenization
    _STOP,
    _split_ident,
    tokenize_queries,
)

# ---------------------------------------------------------------------------
# Re-exports from hybrid_ranking
# ---------------------------------------------------------------------------
from scripts.hybrid_ranking import (
    # RRF
    rrf,
    _scale_rrf_k,
    _adaptive_per_query,
    _normalize_scores,
    # Sparse lexical scoring
    sparse_lex_score,
    # Lexical scoring
    lexical_score,
    lexical_text_score,
    tokenize_queries,
    # Adaptive weights
    _compute_query_stats,
    _adaptive_weights,
    _bm25_token_weights_from_results,
    # MMR diversification
    _mmr_diversify,
    # Micro-span budgeting
    _merge_and_budget_spans,
    # Collection stats
    _get_collection_stats,
    _COLL_STATS_CACHE,
    _COLL_STATS_TTL,
    # Implementation intent detection
    _detect_implementation_intent,
    _IMPL_INTENT_PATTERNS,
)

# ---------------------------------------------------------------------------
# Re-exports from hybrid_expand
# ---------------------------------------------------------------------------
from scripts.hybrid_expand import (
    # Synonyms
    CODE_SYNONYMS,
    # Expansion functions
    expand_queries,
    expand_queries_enhanced,
    _llm_expand_queries,
    _prf_terms_from_results,
    # Availability flag
    SEMANTIC_EXPANSION_AVAILABLE,
)

# Conditionally re-export semantic expansion functions
if SEMANTIC_EXPANSION_AVAILABLE:
    from scripts.hybrid_expand import (
        expand_queries_semantically,
        expand_queries_with_prf,
        get_expansion_stats,
        clear_expansion_cache,
    )
else:
    expand_queries_semantically = None
    expand_queries_with_prf = None
    get_expansion_stats = None
    clear_expansion_cache = None

# ---------------------------------------------------------------------------
# Additional imports for backward compatibility
# ---------------------------------------------------------------------------
try:
    from fastembed import TextEmbedding
except ImportError:
    TextEmbedding = None  # type: ignore

try:
    from scripts.embedder import get_embedding_model as _get_embedding_model
except ImportError:
    _get_embedding_model = None

# Import request deduplication system
try:
    from scripts.deduplication import get_deduplicator, is_duplicate_request
    DEDUPLICATION_AVAILABLE = True
except ImportError:
    DEDUPLICATION_AVAILABLE = False

# Import query optimizer for dynamic EF tuning
try:
    from scripts.query_optimizer import get_query_optimizer, optimize_query
    QUERY_OPTIMIZER_AVAILABLE = True
except ImportError:
    QUERY_OPTIMIZER_AVAILABLE = False

# Import ingest helpers
from scripts.utils import sanitize_vector_name as _sanitize_vector_name
from scripts.ingest_code import ensure_collection as _ensure_collection_raw
from scripts.ingest_code import project_mini as _project_mini

# ---------------------------------------------------------------------------
# Module logger
# ---------------------------------------------------------------------------
logger = logging.getLogger("hybrid_search")

# ---------------------------------------------------------------------------
# Filter sanitization cache (kept here for run_hybrid_search)
# ---------------------------------------------------------------------------
_FILTER_CACHE: Dict[int, Any] = {}
_FILTER_CACHE_LOCK = threading.Lock()
_FILTER_CACHE_MAX = 256

# Cached regex pattern compilation
@lru_cache(maxsize=128)
def _compile_regex(pattern: str, flags: int = 0):
    """Cached regex compilation for repeated patterns."""
    return re.compile(pattern, flags)

# ---------------------------------------------------------------------------
# Lexical helper (used in run_hybrid_search for query sharpening)
# ---------------------------------------------------------------------------
def _split_ident_lex(s: str) -> List[str]:
    """Split identifier into tokens (snake_case and camelCase aware)."""
    parts = re.split(r"[^A-Za-z0-9]+", s)
    out: List[str] = []
    for p in parts:
        if not p:
            continue
        segs = re.findall(r"[A-Z]?[a-z]+|[A-Z]+(?![a-z])|\d+", p)
        out.extend([x for x in segs if x])
    return [x.lower() for x in out if x and x.lower() not in _STOP]


# ---------------------------------------------------------------------------
# Query Intent Classification (for adaptive weights)
# ---------------------------------------------------------------------------
_CONCEPTUAL_KEYWORDS = {"how", "why", "what", "when", "where", "explain", "mechanism", "works", "architecture", "design", "purpose", "difference"}
_IDENTIFIER_PATTERNS = re.compile(r"[_A-Z]|^\w+\.\w+$|^[a-z]+[A-Z]")  # underscore, dot-qualified, camelCase


def _classify_query_intent(query: str) -> str:
    """
    Classify query intent for adaptive scoring.
    
    Returns:
        'conceptual' - boosts dense weight (semantic queries: how/why/what)
        'identifier' - keeps weights as-is (code symbols, function names)
        'mixed' - default balanced weights
    """
    q_lower = query.lower()
    words = set(q_lower.split())
    
    # Conceptual: question words, explanatory queries
    if words & _CONCEPTUAL_KEYWORDS:
        return "conceptual"
    
    # Identifier: has code-like patterns (underscores, camelCase, qualified names)
    if _IDENTIFIER_PATTERNS.search(query):
        return "identifier"
    
    return "mixed"


# ---------------------------------------------------------------------------
# Backward compatibility: _embed_queries_cached alias
# ---------------------------------------------------------------------------
# The function is now in hybrid_embed.py as embed_queries_cached
# Keep the underscore-prefixed alias for any legacy callers


def run_hybrid_search(
    queries: List[str],
    limit: int = 10,
    per_path: int = 1,
    language: str | None = None,
    under: str | None = None,
    kind: str | None = None,
    symbol: str | None = None,
    ext: str | None = None,
    not_filter: str | None = None,
    case: str | None = None,
    path_regex: str | None = None,
    path_glob: str | list[str] | None = None,
    not_glob: str | list[str] | None = None,
    expand: bool = True,
    model: Any = None,
    collection: str | None = None,
    mode: str | None = None,
    repo: str | list[str] | None = None,  # Filter by repo name(s); "*" to disable auto-filter
    per_query: int | None = None,  # Base candidate retrieval per query (default: adaptive)
) -> List[Dict[str, Any]]:
    # Use pooled client instead of creating a new one per request
    client = get_qdrant_client(
        url=os.environ.get("QDRANT_URL", QDRANT_URL),
        api_key=API_KEY
    )
    try:
        return _run_hybrid_search_impl(
            client, queries, limit, per_path, language, under, kind, symbol, ext,
            not_filter, case, path_regex, path_glob, not_glob, expand, model,
            collection, mode, repo, per_query
        )
    finally:
        return_qdrant_client(client)


def _run_hybrid_search_impl(
    client: QdrantClient,
    queries: List[str],
    limit: int,
    per_path: int,
    language: str | None,
    under: str | None,
    kind: str | None,
    symbol: str | None,
    ext: str | None,
    not_filter: str | None,
    case: str | None,
    path_regex: str | None,
    path_glob: str | list[str] | None,
    not_glob: str | list[str] | None,
    expand: bool,
    model: Any,
    collection: str | None,
    mode: str | None,
    repo: str | list[str] | None,
    per_query: int | None,
) -> List[Dict[str, Any]]:
    """Internal implementation of hybrid search with provided client."""
    # Optional timing for debugging (set DEBUG_SEARCH_TIMING=1 to enable)
    _timing_enabled = os.environ.get("DEBUG_SEARCH_TIMING", "").lower() in {"1", "true", "yes", "on"}
    if _timing_enabled:
        import time as _t
        _t0 = _t.perf_counter()
        def _dt(label: str):
            print(f"  [timing] {label}: {(_t.perf_counter() - _t0)*1000:.1f}ms", flush=True)
    else:
        def _dt(label: str):
            pass

    model_name = os.environ.get("EMBEDDING_MODEL", MODEL_NAME)
    if model:
        _model = model
    elif _EMBEDDER_FACTORY:
        _model = _get_embedding_model(model_name)
    else:
        _model = TextEmbedding(model_name=model_name)
    vec_name = _sanitize_vector_name(model_name)

    # Parse Query DSL and merge with explicit args
    raw_queries = list(queries)
    clean_queries, dsl = parse_query_dsl(raw_queries)
    eff_language = language or dsl.get("language")
    eff_under = under or dsl.get("under")
    eff_kind = kind or dsl.get("kind")
    eff_symbol = symbol or dsl.get("symbol")
    eff_ext = ext or dsl.get("ext")
    eff_not = not_filter or dsl.get("not")
    eff_case = case or dsl.get("case") or os.environ.get("HYBRID_CASE", "insensitive")
    # Repo filter: explicit param > DSL > auto-detect from env
    eff_repo = repo or dsl.get("repo")
    # Normalize repo to list for multi-repo support
    if eff_repo and isinstance(eff_repo, str):
        if eff_repo.strip() == "*":
            eff_repo = None  # "*" means search all repos
        else:
            eff_repo = [r.strip() for r in eff_repo.split(",") if r.strip()]
    elif eff_repo and isinstance(eff_repo, (list, tuple)):
        eff_repo = [str(r).strip() for r in eff_repo if str(r).strip() and str(r).strip() != "*"]
        if not eff_repo:
            eff_repo = None
    # Auto-detect repo from env if not specified and auto-filter is enabled
    if eff_repo is None and str(os.environ.get("REPO_AUTO_FILTER", "1")).strip().lower() in {"1", "true", "yes", "on"}:
        auto_repo = os.environ.get("CURRENT_REPO") or os.environ.get("REPO_NAME")
        if auto_repo and auto_repo.strip():
            eff_repo = [auto_repo.strip()]
    eff_path_regex = path_regex

    def _to_list(x):
        if x is None:
            return []
        if isinstance(x, (list, tuple)):
            out = []
            for e in x:
                s = str(e).strip()
                if s:
                    out.append(s)
            return out
        s = str(x).strip()
        return [s] if s else []

    eff_path_globs = _to_list(path_glob)
    eff_not_globs = _to_list(not_glob)

    # Normalize glob patterns: allow repo-relative (e.g., "src/*.py") to match
    # stored absolute paths (e.g., "/work/src/..."). We keep both original and
    # absolute-prefixed variants for matching.
    def _normalize_globs(globs: list[str]) -> list[str]:
        out: list[str] = []
        try:
            for g in (globs or []):
                s = str(g).strip().replace("\\", "/")
                if not s:
                    continue
                out.append(s)
                if not s.startswith("/"):
                    # /work/<slug>/... is common; include both direct /work and slug-aware variants
                    stripped = s.lstrip("/")
                    out.append("/work/" + stripped)
                    out.append("/work/*/" + stripped)
        except Exception:
            pass
        # Dedup while preserving order
        seen = set()
        dedup: list[str] = []
        for g in out:
            if g not in seen:
                dedup.append(g)
                seen.add(g)
        return dedup

    eff_path_globs_norm = _normalize_globs(eff_path_globs)
    eff_not_globs_norm = _normalize_globs(eff_not_globs)

    # Normalize under
    def _norm_under(u: str | None) -> str | None:
        if not u:
            return None
        u = str(u).strip().replace("\\", "/")
        u = "/".join([p for p in u.split("/") if p])
        if not u:
            return None
        if not u.startswith("/"):
            v = "/work/" + u
        else:
            v = "/work/" + u.lstrip("/") if not u.startswith("/work/") else u
        return v

    eff_under = _norm_under(eff_under)

    # Expansion knobs that affect query construction/results (must be part of cache key)
    try:
        llm_max = int(os.environ.get("LLM_EXPAND_MAX", "0") or 0)
    except (ValueError, TypeError):
        llm_max = 0
    try:
        _semantic_enabled = _env_truthy(os.environ.get("SEMANTIC_EXPANSION_ENABLED"), True)
    except Exception:
        _semantic_enabled = True
    try:
        _semantic_max_terms = int(os.environ.get("SEMANTIC_EXPANSION_MAX_TERMS", "3") or 3)
    except (ValueError, TypeError):
        _semantic_max_terms = 3
    _code_signal_syms = os.environ.get("CODE_SIGNAL_SYMBOLS", "").strip()

    # Results cache: return cached results for identical (queries, filters, knobs)
    _USE_CACHE = (MAX_RESULTS_CACHE > 0) and _env_truthy(os.environ.get("HYBRID_RESULTS_CACHE_ENABLED"), True)
    cache_key = None
    if _USE_CACHE:
        try:
            cache_key = (
                "v2",
                tuple(clean_queries),
                int(limit or 0),
                int(per_path or 0),
                str(eff_language or ""),
                str(eff_under or ""),
                str(eff_kind or ""),
                str(eff_symbol or ""),
                str(eff_ext or ""),
                str(eff_not or ""),
                str(eff_case or ""),
                tuple(eff_path_globs_norm or ()),
                tuple(eff_not_globs_norm or ()),
                str(eff_repo or ""),
                str(eff_path_regex or ""),
                bool(expand),
                int(llm_max),
                bool(_semantic_enabled),
                int(_semantic_max_terms),
                str(_code_signal_syms),
                str(vec_name),
                str(_collection()),
                _env_truthy(os.environ.get("HYBRID_ADAPTIVE_WEIGHTS"), True),
                _env_truthy(os.environ.get("HYBRID_MMR"), True),
                str(mode or ""),
            )
        except Exception:
            cache_key = None
        if cache_key is not None:
            if UNIFIED_CACHE_AVAILABLE:
                val = _RESULTS_CACHE.get(cache_key)
                if val is not None:
                    if os.environ.get("DEBUG_HYBRID_SEARCH"):
                        logger.debug("cache hit for hybrid results (unified)")
                    return val
                # Fallback to local in-process dict to ensure deterministic hits (esp. in unit tests)
                try:
                    with _RESULTS_LOCK:
                        if cache_key in _RESULTS_CACHE_OD:
                            if os.environ.get("DEBUG_HYBRID_SEARCH"):
                                logger.debug("cache hit for hybrid results (fallback OD)")
                            return _RESULTS_CACHE_OD[cache_key]
                except Exception:
                    pass
            else:
                with _RESULTS_LOCK:
                    if cache_key in _RESULTS_CACHE:
                        val = _RESULTS_CACHE.pop(cache_key)
                        _RESULTS_CACHE[cache_key] = val
                        if os.environ.get("DEBUG_HYBRID_SEARCH"):
                            logger.debug("cache hit for hybrid results (legacy)")
                        return val

    # Build optional filter
    flt = None
    must = []
    if eff_language:
        must.append(
            models.FieldCondition(
                key="metadata.language", match=models.MatchValue(value=eff_language)
            )
        )
    # Repo filter: supports single repo or list of repos (for related codebases)
    if eff_repo:
        if isinstance(eff_repo, list) and len(eff_repo) == 1:
            must.append(
                models.FieldCondition(
                    key="metadata.repo", match=models.MatchValue(value=eff_repo[0])
                )
            )
        elif isinstance(eff_repo, list) and len(eff_repo) > 1:
            # Multiple repos: use MatchAny for OR logic
            must.append(
                models.FieldCondition(
                    key="metadata.repo", match=models.MatchAny(any=eff_repo)
                )
            )
        elif isinstance(eff_repo, str):
            must.append(
                models.FieldCondition(
                    key="metadata.repo", match=models.MatchValue(value=eff_repo)
                )
            )
    if eff_under:
        must.append(
            models.FieldCondition(
                key="metadata.path_prefix", match=models.MatchValue(value=eff_under)
            )
        )
    if eff_kind:
        must.append(
            models.FieldCondition(
                key="metadata.kind", match=models.MatchValue(value=eff_kind)
            )
        )
    if eff_symbol:
        must.append(
            models.FieldCondition(
                key="metadata.symbol", match=models.MatchValue(value=eff_symbol)
            )
        )

    # After attempting cache get, run deduplication. If duplicate, serve cached result if present.
    if DEDUPLICATION_AVAILABLE:
        request_data = {
            'queries': queries,
            'limit': limit,
            'per_path': per_path,
            'language': language,
            'under': under,
            'kind': kind,
            'symbol': symbol,
            'ext': ext,
            'not': not_filter,
            'case': case,
            'path_regex': path_regex,
            'path_glob': path_glob,
            'not_glob': not_glob,
            'expand': expand,
            'collection': _collection(),
            'vector_name': vec_name,
            'mode': mode,
        }
        is_duplicate, similar_fp = is_duplicate_request(request_data)
        if is_duplicate:
            # Prefer serving from cache on duplicate
            if cache_key is not None:
                if UNIFIED_CACHE_AVAILABLE:
                    val = _RESULTS_CACHE.get(cache_key)
                    if val is not None:
                        if os.environ.get("DEBUG_HYBRID_SEARCH"):
                            logger.debug("duplicate served from cache (unified)")
                        return val
                    try:
                        with _RESULTS_LOCK:
                            if cache_key in _RESULTS_CACHE_OD:
                                if os.environ.get("DEBUG_HYBRID_SEARCH"):
                                    logger.debug("duplicate served from cache (fallback OD)")
                                return _RESULTS_CACHE_OD[cache_key]
                    except Exception:
                        pass
                else:
                    with _RESULTS_LOCK:
                        if cache_key in _RESULTS_CACHE:
                            val = _RESULTS_CACHE.pop(cache_key)
                            _RESULTS_CACHE[cache_key] = val
                            if os.environ.get("DEBUG_HYBRID_SEARCH"):
                                logger.debug("duplicate served from cache (legacy)")
                            return val
            if os.environ.get("DEBUG_HYBRID_SEARCH"):
                logger.debug("Duplicate without cache; bypassing dedup and continuing search")


    # Add ext filter (file extension) - server-side when possible
    if eff_ext:
        ext_clean = eff_ext.lower().lstrip(".")
        must.append(
            models.FieldCondition(
                key="metadata.ext", match=models.MatchValue(value=ext_clean)
            )
        )

    # Add not filter (simple text exclusion on path) - server-side when possible
    must_not = []
    if eff_not:
        # Try MatchText for substring exclusion; fallback to post-filter if unsupported
        try:
            must_not.append(
                models.FieldCondition(
                    key="metadata.path", match=models.MatchText(text=eff_not)
                )
            )
        except Exception:
            # Will be handled by post-filter
            pass

    flt = models.Filter(must=must, must_not=must_not) if (must or must_not) else None
    flt = _sanitize_filter_obj(flt)


    # Build query list (LLM-assisted first, then synonym expansion)
    qlist = list(clean_queries)
    # Preserve the original (pre-expansion) queries for IDF/BM25 weighting
    base_queries = list(clean_queries)

    # Filter-only mode: derive implicit queries from DSL tokens
    # e.g., "lang:python" -> add "python" as a query term for ranking
    if not qlist or (len(qlist) == 1 and not qlist[0].strip()):
        qlist = []  # Start fresh
        if eff_language:
            qlist.append(eff_language)
        if eff_symbol:
            qlist.append(eff_symbol)
        if eff_kind:
            qlist.append(eff_kind)
        if eff_ext:
            # Add extension without dot as query term
            ext_term = eff_ext.lstrip(".")
            if ext_term:
                qlist.append(ext_term)
        if eff_under:
            # Add path segments as query terms
            parts = [p for p in str(eff_under).replace("\\", "/").split("/") if p]
            for p in parts[-2:]:  # Last 2 path segments
                if p and p not in qlist:
                    qlist.append(p)
    if llm_max > 0:
        _llm_more = _llm_expand_queries(qlist, eff_language, max_new=llm_max)
        for s in _llm_more:
            if s and s not in qlist:
                qlist.append(s)
    if expand:
        # Use enhanced expansion with semantic similarity if available
        if SEMANTIC_EXPANSION_AVAILABLE:
            qlist = expand_queries_enhanced(
                qlist, eff_language,
                max_extra=max(2, _semantic_max_terms),
                client=client,
                model=_model,
                collection=_collection(collection)
            )
        else:
            qlist = expand_queries(qlist, eff_language)

    # Query sharpening: derive basename tokens from path_glob to steer retrieval/gating
    try:
        if eff_path_globs or eff_path_globs_norm:
            def _bn(p: str) -> str:
                s = str(p or "").replace("\\", "/").strip()
                # drop any trailing slashes and take last segment
                parts = [t for t in s.split("/") if t]
                return parts[-1] if parts else ""
            globs_src = list(eff_path_globs or []) + list(eff_path_globs_norm or [])
            basenames = []
            for g in globs_src:
                b = _bn(g)
                if b and b not in basenames:
                    basenames.append(b)
            for b in basenames:
                if b and b not in qlist:
                    qlist.append(b)
                # also add stem (filename without extension) as a lexical hint
                stem = b.rsplit(".", 1)[0] if "." in b else b
                if stem and stem not in qlist:
                    qlist.append(stem)
            # Add short path segments (e.g., "scripts/hybrid_search") to steer lexical hashing
            for g in globs_src:
                s = str(g or "").replace("\\", "/").strip()
                parts = [t for t in s.split("/") if t]
                if len(parts) >= 2:
                    last2 = "/".join(parts[-2:])
                    if last2 and last2 not in qlist:
                        qlist.append(last2)
                if len(parts) >= 3:
                    last3 = "/".join(parts[-3:])
                    if last3 and last3 not in qlist:
                        qlist.append(last3)
    except Exception:
        pass

    # --- Code signal symbols: add extracted symbols from query analysis ---
    # These are passed via CODE_SIGNAL_SYMBOLS env var from repo_search
    try:
        _code_signal_syms = os.environ.get("CODE_SIGNAL_SYMBOLS", "").strip()
        if _code_signal_syms:
            for sym in _code_signal_syms.split(","):
                sym = sym.strip()
                if sym and len(sym) > 1 and sym not in qlist:
                    qlist.append(sym)
    except Exception:
        pass

    # === Large codebase scaling (automatic) ===
    _coll_stats = _get_collection_stats(client, _collection(collection))
    # IMPORTANT: Use actual Qdrant points_count, not corpus doc count.
    # With chunked indexing (multiple points per doc), points_count >> doc_count.
    # Fusion depth must scale with points_count to prevent recall collapse.
    _coll_size = _coll_stats.get("points_count", 0)
    _has_filters = bool(eff_language or eff_repo or eff_under or eff_kind or eff_symbol or eff_ext)

    # Scale RRF k for better score discrimination at scale
    _scaled_rrf_k = _scale_rrf_k(RRF_K, _coll_size)

    # Adaptive per_query: retrieve more candidates from larger collections
    # Scales based on points_count (not doc count) to maintain recall with chunking.
    # Use explicit per_query if provided, then check HYBRID_PER_QUERY env, otherwise compute adaptively
    _per_query_env = os.environ.get("HYBRID_PER_QUERY", "").strip()
    _per_query_env_int: int | None = None
    if _per_query_env:
        try:
            _per_query_env_int = int(_per_query_env)
        except ValueError:
            _per_query_env_int = None

    if per_query is not None:
        try:
            _scaled_per_query = max(1, int(per_query))
        except (TypeError, ValueError):
            _scaled_per_query = _adaptive_per_query(max(24, limit), _coll_size, _has_filters)
    elif _per_query_env_int is not None and _per_query_env_int > 0:
        _scaled_per_query = _per_query_env_int
    else:
        _scaled_per_query = _adaptive_per_query(max(24, limit), _coll_size, _has_filters)

    if os.environ.get("DEBUG_HYBRID_SEARCH"):
        logger.debug(f"Hybrid search scaling: size={_coll_size}, rrf_k={_scaled_rrf_k}, per_query={_scaled_per_query}")

    # Local RRF function using scaled k
    def _scaled_rrf(rank: int) -> float:
        return 1.0 / (_scaled_rrf_k + rank)

    _dt("setup+expand")

    # Lexical vector query (with scaled retrieval)
    # Use sparse vectors when LEX_SPARSE_MODE is enabled for lossless matching
    score_map: Dict[str, Dict[str, Any]] = {}
    _used_sparse_lex = False  # Track if we actually used sparse (for scoring)
    try:
        if LEX_SPARSE_MODE:
            sparse_vec = lex_sparse_vector(qlist)
            lex_results = sparse_lex_query(client, sparse_vec, flt, _scaled_per_query, collection)
            # Fallback to dense lex if sparse returned empty (collection may not have sparse index)
            if not lex_results:
                lex_vec = lex_hash_vector(qlist)
                lex_results = lex_query(client, lex_vec, flt, _scaled_per_query, collection)
                if lex_results:
                    logger.debug("LEX_SPARSE_MODE enabled but sparse query returned empty; fell back to dense lex")
            else:
                _used_sparse_lex = True  # Actually used sparse vectors
        else:
            lex_vec = lex_hash_vector(qlist)
            lex_results = lex_query(client, lex_vec, flt, _scaled_per_query, collection)
    except Exception as e:
        # On sparse query failure, try falling back to dense lex
        if LEX_SPARSE_MODE:
            try:
                lex_vec = lex_hash_vector(qlist)
                lex_results = lex_query(client, lex_vec, flt, _scaled_per_query, collection)
                logger.warning("LEX_SPARSE_MODE sparse query failed (%s); fell back to dense lex", e)
            except Exception:
                lex_results = []
        else:
            lex_results = []

    _dt("lex_query")

    # Per-query adaptive weights (default ON, gentle clamps)
    _USE_ADAPT = _env_truthy(os.environ.get("HYBRID_ADAPTIVE_WEIGHTS"), True)
    if _USE_ADAPT:
        try:
            _AD_DENSE_W, _AD_LEX_VEC_W, _AD_LEX_TEXT_W = _adaptive_weights(_compute_query_stats(qlist))
        except Exception:
            _AD_DENSE_W, _AD_LEX_VEC_W, _AD_LEX_TEXT_W = DENSE_WEIGHT, LEX_VECTOR_WEIGHT, LEXICAL_WEIGHT
    else:
        _AD_DENSE_W, _AD_LEX_VEC_W, _AD_LEX_TEXT_W = DENSE_WEIGHT, LEX_VECTOR_WEIGHT, LEXICAL_WEIGHT

    # Intent-based dense boost: conceptual queries need more semantic weight
    _query_intent = _classify_query_intent(" ".join(qlist))
    _CONCEPTUAL_DENSE_BOOST = float(os.environ.get("CONCEPTUAL_DENSE_BOOST", "3.0"))
    if _query_intent == "conceptual":
        _AD_DENSE_W *= _CONCEPTUAL_DENSE_BOOST  # Boost dense for semantic queries

    for rank, p in enumerate(lex_results, 1):
        pid = str(p.id)
        score_map.setdefault(
            pid,
            {
                "pt": p,
                "s": 0.0,
                "d": 0.0,
                "lx": 0.0,
                "sym_sub": 0.0,
                "sym_eq": 0.0,
                "fname": 0.0,
                "core": 0.0,
                "vendor": 0.0,
                "langb": 0.0,
                "rec": 0.0,
                "test": 0.0,
            },
        )
        # Sparse vectors: use actual similarity score (preserves match quality signal)
        # Dense vectors: use RRF rank (backwards compatible)
        if _used_sparse_lex:
            _lex_w = _AD_LEX_VEC_W if _USE_ADAPT else LEX_VECTOR_WEIGHT
            lxs = sparse_lex_score(float(getattr(p, 'score', 0) or 0), weight=_lex_w, rrf_k=_scaled_rrf_k)
        else:
            lxs = (_AD_LEX_VEC_W * _scaled_rrf(rank)) if _USE_ADAPT else (LEX_VECTOR_WEIGHT * _scaled_rrf(rank))
        score_map[pid]["lx"] += lxs
        score_map[pid]["s"] += lxs

    _dt("lex_score_map")

    # Dense queries - filter out empty strings for filter-only mode (e.g., "lang:python")
    qlist_for_embed = [q for q in qlist if q and q.strip()]
    embedded = _embed_queries_cached(_model, qlist_for_embed) if qlist_for_embed else []
    _dt("embed_queries")
    # Ensure collection schema is compatible with current search settings (named vectors)
    try:
        if embedded:
            dim = len(embedded[0])
            _ensure_collection(client, _collection(collection), dim, vec_name)
    except Exception:
        pass
    # Optional gate-first using mini vectors to restrict dense search to candidates
    # Adaptive gating: disable for short/ambiguous queries to avoid over-filtering
    flt_gated = flt
    try:
        gate_first = str(os.environ.get("REFRAG_GATE_FIRST", "0")).strip().lower() in {
            "1",
            "true",
            "yes",
            "on",
        }
        refrag_on = str(os.environ.get("REFRAG_MODE", "")).strip().lower() in {
            "1",
            "true",
            "yes",
            "on",
        }
        cand_n = int(os.environ.get("REFRAG_CANDIDATES", "200") or 200)
    except (ValueError, TypeError):
        gate_first, refrag_on, cand_n = False, False, 200

    # Adaptive mini-gate: disable for queries that are too short or lack strong identifiers
    should_bypass_gate = False
    if gate_first and refrag_on:
        # Check query characteristics
        try:
            # Count total tokens across queries
            total_tokens = sum(len(_split_ident(q)) for q in qlist)
            # Check for strong identifiers (ALL_CAPS, camelCase, or has underscore)
            has_strong_id = any(
                any(t.isupper() or "_" in t or any(c.isupper() for c in t[1:]) and any(c.islower() for c in t)
                    for t in _split_ident(q))
                for q in qlist
            )
            # If query is very short (<3 tokens) and has no strong identifiers, bypass gate
            if total_tokens < 3 and not has_strong_id:
                should_bypass_gate = True
                if os.environ.get("DEBUG_HYBRID_SEARCH"):
                    logger.debug(f"Adaptive gate bypass (short query, tokens={total_tokens}, strong_id={has_strong_id})")
            # If we have strict filters (language, under, symbol, ext), relax candidate count
            if eff_language or eff_under or eff_symbol or eff_ext:
                cand_n = max(cand_n, limit * 5)
                if os.environ.get("DEBUG_HYBRID_SEARCH"):
                    logger.debug(f"Adaptive gate relaxed candidate count to {cand_n} due to filters")
        except Exception:
            pass

    _gate_first_ran = False
    if gate_first and refrag_on and not should_bypass_gate:
        try:
            # ReFRAG gate-first: Use MINI vectors to prefilter candidates
            mini_queries = [_project_mini(list(v), MINI_VEC_DIM) for v in embedded]

            # Get top candidates using MINI vectors (fast prefilter)
            candidate_ids = set()
            for mv in mini_queries:
                mini_results = dense_query(client, MINI_VECTOR_NAME, mv, flt, cand_n, collection)
                for result in mini_results:
                    if hasattr(result, 'id'):
                        candidate_ids.add(result.id)

            if candidate_ids:
                # Server-side gating without requiring payload fields: prefer HasIdCondition
                from qdrant_client import models as _models
                try:
                    gating_cond = _models.HasIdCondition(has_id=list(candidate_ids))
                    gating_kind = "has_id"
                except Exception:
                    # Fallback to pid_str if HasIdCondition unavailable
                    id_vals = [str(cid) for cid in candidate_ids]
                    gating_cond = _models.FieldCondition(
                        key="pid_str",
                        match=_models.MatchAny(any=id_vals),
                    )
                    gating_kind = "pid_str"
                if flt is None:
                    flt_gated = _models.Filter(must=[gating_cond])
                else:
                    must = list(flt.must or [])
                    must.append(gating_cond)
                    flt_gated = _models.Filter(must=must, should=flt.should, must_not=flt.must_not)
                if os.environ.get("DEBUG_HYBRID_SEARCH"):
                    logger.debug(f"ReFRAG gate-first (server-side-{gating_kind}): {len(candidate_ids)} candidates")
                    logger.debug(f"flt_gated.must has {len(flt_gated.must or [])} conditions")
                    logger.debug(f"flt_gated.must_not has {len(flt_gated.must_not or [])} conditions")
            else:
                # No candidates -> no gating
                flt_gated = flt
            # Mark gate-first as successful only after all logic completes
            _gate_first_ran = True
        except Exception as e:
            if os.environ.get("DEBUG_HYBRID_SEARCH"):
                logger.debug(f"ReFRAG gate-first failed: {e}, proceeding without gating")
            # Fallback to normal search (no gating)
            flt_gated = flt
    else:
        flt_gated = flt

    # Sanitize filter: if empty, drop it to avoid Qdrant 400s on invalid filters
    try:
        if flt_gated is not None:
            _m = [c for c in (getattr(flt_gated, "must", None) or []) if c is not None]
            _s = [c for c in (getattr(flt_gated, "should", None) or []) if c is not None]
            _mn = [c for c in (getattr(flt_gated, "must_not", None) or []) if c is not None]
            if not _m and not _s and not _mn:
                flt_gated = None
    except Exception:
        pass

    flt_gated = _sanitize_filter_obj(flt_gated)

    # Parallel dense query execution for multiple queries (threshold >= 4 to avoid thread overhead for small N)
    try:
        _parallel_threshold = int(os.environ.get("PARALLEL_DENSE_THRESHOLD", "4") or 4)
    except (ValueError, TypeError):
        logger.warning(
            "Invalid PARALLEL_DENSE_THRESHOLD value %r, using default 4",
            os.environ.get("PARALLEL_DENSE_THRESHOLD"),
        )
        _parallel_threshold = 4
    if len(embedded) >= _parallel_threshold and os.environ.get("PARALLEL_DENSE_QUERIES", "1") == "1":
        executor = _get_query_executor()
        futures = [
            executor.submit(
                dense_query,
                client,
                vec_name,
                v,
                flt_gated,
                _scaled_per_query,
                collection,
                query_text=queries[i] if i < len(queries) else None,
            )
            for i, v in enumerate(embedded)
        ]
        result_sets: List[List[Any]] = [f.result() for f in futures]
    else:
        result_sets: List[List[Any]] = [
            dense_query(
                client,
                vec_name,
                v,
                flt_gated,
                _scaled_per_query,
                collection,
                query_text=queries[i] if i < len(queries) else None,
            )
            for i, v in enumerate(embedded)
        ]
    _dt("dense_queries")
    if os.environ.get("DEBUG_HYBRID_SEARCH"):
        total_dense_results = sum(len(rs) for rs in result_sets)
        logger.debug(f"Dense query returned {total_dense_results} total results across {len(result_sets)} queries")

    _dt("dense_score_map")

    # Optional ReFRAG-style mini-vector gating: add compact-vector RRF if enabled
    try:
        if not _gate_first_ran and os.environ.get("REFRAG_MODE", "").strip().lower() in {
            "1",
            "true",
            "yes",
            "on",
        }:
            try:
                mini_queries = [_project_mini(list(v), MINI_VEC_DIM) for v in embedded]
                mini_sets: List[List[Any]] = [
                    dense_query(client, MINI_VECTOR_NAME, mv, flt, _scaled_per_query, collection)
                    for mv in mini_queries
                ]
                for res in mini_sets:
                    for rank, p in enumerate(res, 1):
                        pid = str(p.id)
                        score_map.setdefault(
                            pid,
                            {
                                "pt": p,
                                "s": 0.0,
                                "d": 0.0,
                                "lx": 0.0,
                                "sym_sub": 0.0,
                                "sym_eq": 0.0,
                                "fname": 0.0,
                                "core": 0.0,
                                "vendor": 0.0,
                                "langb": 0.0,
                                "rec": 0.0,
                                "test": 0.0,
                            },
                        )
                        dens = float(HYBRID_MINI_WEIGHT) * _scaled_rrf(rank)
                        score_map[pid]["d"] += dens
                        score_map[pid]["s"] += dens
            except Exception:
                pass
    except Exception:
        pass

    # Enhanced PRF with semantic similarity
    if SEMANTIC_EXPANSION_AVAILABLE and score_map:
        # Local PRF dense weight (fallback if not set later)
        try:
            prf_dw = float(os.environ.get("PRF_DENSE_WEIGHT", "0.4") or 0.4)
        except Exception:
            prf_dw = 0.4

        try:
            # Get top results for PRF context (sorted by score, not arbitrary dict order)
            sorted_items = sorted(score_map.items(), key=lambda x: x[1]["s"], reverse=True)
            top_results = [rec["pt"] for _, rec in sorted_items[:8]]

            if top_results:
                semantic_prf_terms = expand_queries_with_prf(
                    clean_queries, top_results, _model, max_expansions=4
                )

                # Create PRF queries using semantic terms
                semantic_prf_qs = []
                for term in semantic_prf_terms:
                    base = clean_queries[0] if clean_queries else (qlist[0] if qlist else "")
                    cand = (base + " " + term).strip()
                    if cand and cand not in qlist and cand not in semantic_prf_qs:
                        semantic_prf_qs.append(cand)
                        if len(semantic_prf_qs) >= 3:  # Limit semantic PRF queries
                            break

                if semantic_prf_qs:
                    # Dense semantic PRF pass
                    embedded_sem_prf = _embed_queries_cached(_model, semantic_prf_qs)
                    result_sets_sem_prf: List[List[Any]] = [
                        dense_query(client, vec_name, v, flt, max(8, limit // 3 or 4), collection)
                        for v in embedded_sem_prf
                    ]
                    for res_sem_prf in result_sets_sem_prf:
                        for rank, p in enumerate(res_sem_prf, 1):
                            pid = str(p.id)
                            score_map.setdefault(
                                pid,
                                {
                                    "pt": p,
                                    "s": 0.0,
                                    "d": 0.0,
                                    "lx": 0.0,
                                    "sym_sub": 0.0,
                                    "sym_eq": 0.0,
                                    "fname": 0.0,
                                    "core": 0.0,
                                    "vendor": 0.0,
                                    "langb": 0.0,
                                    "rec": 0.0,
                                    "test": 0.0,
                                },
                            )
                            # Lower weight for semantic PRF to avoid over-diversification
                            dens = 0.3 * prf_dw * _scaled_rrf(rank)
                            score_map[pid]["d"] += dens
                            score_map[pid]["s"] += dens

                    if os.environ.get("DEBUG_HYBRID_SEARCH"):
                        logger.debug(f"Semantic PRF added {len(semantic_prf_qs)} queries with terms: {semantic_prf_terms}")

        except Exception as e:
            if os.environ.get("DEBUG_HYBRID_SEARCH"):
                logger.debug(f"Semantic PRF failed: {e}")

    lex_results2: List[Any] = []

    # Pseudo-Relevance Feedback (default-on): mine top terms from current results and run a light second pass
    try:
        prf_enabled = _env_truthy(os.environ.get("PRF_ENABLED"), True)
    except (ValueError, TypeError):
        prf_enabled = True

    # Lightweight BM25-style lexical boost (default ON)
    try:
        _USE_BM25 = _env_truthy(os.environ.get("HYBRID_BM25"), True)
    except Exception:
        _USE_BM25 = True
    try:
        _BM25_W = float(os.environ.get("HYBRID_BM25_WEIGHT", "0.2") or 0.2)
    except Exception:
        _BM25_W = 0.2
    _bm25_tok_w = (
        _bm25_token_weights_from_results(
            qlist,
            (lex_results or []) + (lex_results2 or []),
            base_phrases=base_queries,
        )
        if _USE_BM25
        else {}
    )

    if prf_enabled and score_map:
        try:
            top_docs = int(os.environ.get("PRF_TOP_DOCS", "8") or 8)
        except (ValueError, TypeError):
            top_docs = 8
        try:
            max_terms = int(os.environ.get("PRF_MAX_TERMS", "6") or 6)
        except (ValueError, TypeError):
            max_terms = 6
        try:
            extra_q = int(os.environ.get("PRF_EXTRA_QUERIES", "4") or 4)
        except (ValueError, TypeError):
            extra_q = 4
        try:
            prf_dw = float(os.environ.get("PRF_DENSE_WEIGHT", "0.4") or 0.4)
        except (ValueError, TypeError):
            prf_dw = 0.4
        try:
            prf_lw = float(os.environ.get("PRF_LEX_WEIGHT", "0.6") or 0.6)
        except (ValueError, TypeError):
            prf_lw = 0.6
        terms = _prf_terms_from_results(
            score_map, top_docs=top_docs, max_terms=max_terms
        )
        base = clean_queries[0] if clean_queries else (qlist[0] if qlist else "")
        prf_qs: List[str] = []
        for t in terms:
            cand = (base + " " + t).strip()
            if cand and cand not in qlist and cand not in prf_qs:
                prf_qs.append(cand)
                if len(prf_qs) >= extra_q:
                    break
        if prf_qs:
            # Lexical PRF pass (use sparse when enabled, with fallback)
            _prf_limit = max(12, limit // 2 or 6)
            try:
                if LEX_SPARSE_MODE:
                    sparse_vec2 = lex_sparse_vector(prf_qs)
                    lex_results2 = sparse_lex_query(client, sparse_vec2, flt, _prf_limit, collection)
                    if not lex_results2:
                        lex_vec2 = lex_hash_vector(prf_qs)
                        lex_results2 = lex_query(client, lex_vec2, flt, _prf_limit, collection)
                else:
                    lex_vec2 = lex_hash_vector(prf_qs)
                    lex_results2 = lex_query(client, lex_vec2, flt, _prf_limit, collection)
            except Exception:
                if LEX_SPARSE_MODE:
                    try:
                        lex_vec2 = lex_hash_vector(prf_qs)
                        lex_results2 = lex_query(client, lex_vec2, flt, _prf_limit, collection)
                    except Exception:
                        lex_results2 = []
                else:
                    lex_results2 = []
            for rank, p in enumerate(lex_results2, 1):
                pid = str(p.id)
                score_map.setdefault(
                    pid,
                    {
                        "pt": p,
                        "s": 0.0,
                        "d": 0.0,
                        "lx": 0.0,
                        "sym_sub": 0.0,
                        "sym_eq": 0.0,
                        "fname": 0.0,
                        "core": 0.0,
                        "vendor": 0.0,
                        "langb": 0.0,
                        "rec": 0.0,
                        "test": 0.0,
                    },
                )
                lxs = prf_lw * _scaled_rrf(rank)
                score_map[pid]["lx"] += lxs
                score_map[pid]["s"] += lxs
            # Dense PRF pass
            try:
                embedded2 = _embed_queries_cached(_model, prf_qs)
                _prf_per_query = max(12, _scaled_per_query // 2)
                result_sets2: List[List[Any]] = [
                    dense_query(client, vec_name, v, flt, _prf_per_query, collection)
                    for v in embedded2
                ]
                for res2 in result_sets2:
                    for rank, p in enumerate(res2, 1):
                        pid = str(p.id)
                        score_map.setdefault(
                            pid,
                            {
                                "pt": p,
                                "s": 0.0,
                                "d": 0.0,
                                "lx": 0.0,
                                "sym_sub": 0.0,
                                "sym_eq": 0.0,
                                "fname": 0.0,
                                "core": 0.0,
                                "vendor": 0.0,
                                "langb": 0.0,
                                "rec": 0.0,
                                "test": 0.0,
                            },
                        )
                        dens = prf_dw * _scaled_rrf(rank)
                        score_map[pid]["d"] += dens
                        score_map[pid]["s"] += dens
            except Exception:
                pass
    _dt("prf_passes")

    # Add dense scores (with scaled RRF)
    for res in result_sets:
        for rank, p in enumerate(res, 1):
            pid = str(p.id)
            score_map.setdefault(
                pid,
                {
                    "pt": p,
                    "s": 0.0,
                    "d": 0.0,
                    "lx": 0.0,
                    "sym_sub": 0.0,
                    "sym_eq": 0.0,
                    "fname": 0.0,
                    "core": 0.0,
                    "vendor": 0.0,
                    "langb": 0.0,
                    "rec": 0.0,
                    "test": 0.0,
                },
            )
            dens = (_AD_DENSE_W * _scaled_rrf(rank)) if _USE_ADAPT else (DENSE_WEIGHT * _scaled_rrf(rank))
            score_map[pid]["d"] += dens
            score_map[pid]["s"] += dens

    # Lexical + boosts
    timestamps: List[int] = []
    # Mode-aware tweaks for implementation/docs weighting. Modes:
    # - None / "code_first": full IMPLEMENTATION_BOOST and DOCUMENTATION_PENALTY
    # - "balanced": keep impl boost, halve doc penalty
    # - "docs_first": reduce impl boost slightly and disable doc penalty
    eff_mode = (mode or "").strip().lower()
    impl_boost = IMPLEMENTATION_BOOST
    doc_penalty = DOCUMENTATION_PENALTY
    test_penalty = TEST_FILE_PENALTY
    cfg_penalty = CONFIG_FILE_PENALTY
    # Conditional test penalty: disable when query targets tests (pytest/fixture/mock etc.)
    _test_intent_keywords = {"test", "pytest", "fixture", "unittest", "mock", "spec", "conftest"}
    _query_lower = " ".join(qlist).lower()
    _base_query = clean_queries[0] if clean_queries else (_query_lower.strip())
    # Intent-based config/docs handling: only penalize these when query is clearly code-first.
    _docs_intent_keywords = {
        "readme", "docs", "documentation", "guide", "tutorial", "how to", "how-to",
        "usage", "example", "examples", "reference", "faq", "troubleshooting",
    }
    _config_intent_keywords = {
        "config", "configuration", "settings", "env", "dotenv",
        "yaml", "yml", "json", "toml", "ini",
        "docker", "compose", "kubernetes", "k8s", "helm", "chart", "manifest", "deployment",
        "workflow", "github actions", "ci", "pipeline",
    }
    _is_docs_query = any(kw in _query_lower for kw in _docs_intent_keywords)
    _is_config_query = any(kw in _query_lower for kw in _config_intent_keywords)
    if _is_docs_query and not eff_mode:
        # Prefer docs when the query explicitly asks for docs/README/guide.
        doc_penalty = 0.0
        impl_boost = IMPLEMENTATION_BOOST * 0.5
    if _is_config_query:
        cfg_penalty = 0.0
    _is_test_query = any(kw in _query_lower for kw in _test_intent_keywords)
    if _is_test_query:
        test_penalty = 0.0  # User wants test files, don't penalize them
    elif (not _is_docs_query) and _detect_implementation_intent(qlist):
        # Query intent detection: boost implementation files more when query signals code search
        impl_boost += INTENT_IMPL_BOOST
        # Also increase test/doc penalties when user clearly wants implementation
        test_penalty += INTENT_IMPL_BOOST
        doc_penalty += INTENT_IMPL_BOOST * 0.5
    if eff_mode in {"balanced"}:
        doc_penalty = DOCUMENTATION_PENALTY * 0.5
    elif eff_mode in {"docs_first", "docs-first", "docs"}:
        impl_boost = IMPLEMENTATION_BOOST * 0.5
        doc_penalty = 0.0

    # Precompute query tokens once for the boost loop (avoid re-tokenizing per item)
    _precomp_tokens = tokenize_queries(qlist)

    for pid, rec in list(score_map.items()):
        payload = rec["pt"].payload or {}
        base_md = payload.get("metadata") or {}
        # Merge top-level pseudo/tags into the view passed to lexical_score so
        # HYBRID_PSEUDO_BOOST can see index-time GLM/llamacpp labels.
        md = dict(base_md)
        if "pseudo" in payload:
            md["pseudo"] = payload["pseudo"]
        if "tags" in payload:
            md["tags"] = payload["tags"]

        if _USE_ADAPT:
            lx = lexical_text_score(
                qlist,
                md,
                weight=_AD_LEX_TEXT_W,
                token_weights=_bm25_tok_w,
                bm25_weight=_BM25_W,
                rrf_k=_scaled_rrf_k,
                _precomputed_tokens=_precomp_tokens,
            )
        else:
            lx = lexical_text_score(
                qlist,
                md,
                weight=LEXICAL_WEIGHT,
                token_weights=_bm25_tok_w,
                bm25_weight=_BM25_W,
                rrf_k=_scaled_rrf_k,
                _precomputed_tokens=_precomp_tokens,
            )
        rec["lx"] += lx
        rec["s"] += lx
        ts = md.get("last_modified_at") or md.get("ingested_at")
        if isinstance(ts, int):
            timestamps.append(ts)
        sym = str(md.get("symbol") or "").lower()
        sym_path = str(md.get("symbol_path") or "").lower()
        sym_text = f"{sym} {sym_path}"
        # Pre-split symbol into parts for token-level matching (camelCase/snake_case)
        sym_parts = set(p.lower() for p in _split_ident(md.get("symbol") or "") if len(p) >= 2)
        for q in qlist:
            ql = q.lower()
            if not ql:
                continue
            if ql in sym_text:
                rec["sym_sub"] += SYMBOL_BOOST
                rec["s"] += SYMBOL_BOOST
            # Exact match: full symbol OR any split part matches query
            if ql == sym or ql == sym_path or ql in sym_parts:
                rec["sym_eq"] += SYMBOL_EQUALITY_BOOST
                rec["s"] += SYMBOL_EQUALITY_BOOST
        path = str(md.get("path") or "")
        # Filename boost: production-grade matching (handles snake/camel/kebab, acronyms, etc.)
        if FNAME_BOOST > 0.0 and path:
            try:
                from scripts.rerank_recursive.utils import _compute_fname_boost as _compute_fname_boost  # type: ignore
                fname_boost = float(_compute_fname_boost(_base_query, md, float(FNAME_BOOST)))
                if fname_boost > 0:
                    rec["fname"] += fname_boost
                    rec["s"] += fname_boost
            except Exception:
                pass
        if CORE_FILE_BOOST > 0.0 and path and is_core_file(path):
            rec["core"] += CORE_FILE_BOOST
            rec["s"] += CORE_FILE_BOOST
        if VENDOR_PENALTY > 0.0 and path and is_vendor_path(path):
            rec["vendor"] -= VENDOR_PENALTY
            rec["s"] -= VENDOR_PENALTY
        if test_penalty > 0.0 and path and is_test_file(path):
            rec["test"] -= test_penalty
            rec["s"] -= test_penalty

        # Additional file-type weighting
        path_lower = path.lower()
        ext = ("." + path_lower.rsplit(".", 1)[-1]) if "." in path_lower else ""
        # Penalize config/metadata files
        if cfg_penalty > 0.0 and path:
            if ext in {".json", ".yml", ".yaml", ".toml", ".ini"} or "/.codebase/" in path_lower or "/.kiro/" in path_lower:
                rec["cfg"] = float(rec.get("cfg", 0.0)) - cfg_penalty
                rec["s"] -= cfg_penalty
        # Boost likely implementation files (mode-aware)
        if impl_boost > 0.0 and path:
            if ext in {".py", ".ts", ".tsx", ".js", ".jsx", ".go", ".java", ".rs", ".rb", ".php", ".cs", ".cpp", ".c", ".hpp", ".h"}:
                rec["impl"] = float(rec.get("impl", 0.0)) + impl_boost
                rec["s"] += impl_boost
        # Penalize docs (README/docs/markdown) relative to implementation files (mode-aware)
        if doc_penalty > 0.0 and path:
            if (
                "readme" in path_lower
                or "/docs/" in path_lower
                or "/documentation/" in path_lower
                or path_lower.endswith(".md")
            ):
                rec["doc"] = float(rec.get("doc", 0.0)) - doc_penalty
                rec["s"] -= doc_penalty

        if LANG_MATCH_BOOST > 0.0 and path and eff_language:
            lang = str(eff_language).lower()
            md_lang = str((md.get("language") or "").lower())
            if (lang and md_lang and md_lang == lang) or lang_matches_path(lang, path):
                rec["langb"] += LANG_MATCH_BOOST
                rec["s"] += LANG_MATCH_BOOST

        # Memory blending: apply penalty to memory-like entries to prevent swamping code results
        # Only apply if query doesn't explicitly ask for memories/notes
        kind = str(md.get("kind") or "").lower()
        if kind == "memory":
            qlow = " ".join(qlist).lower()
            is_memory_query = any(w in qlow for w in ["remember", "note", "recall", "memo", "stored"])
            if not is_memory_query:
                # Apply penalty and cap at 1 memory result unless explicitly requested
                memory_penalty = float(os.environ.get("HYBRID_MEMORY_PENALTY", "0.15") or 0.15)
                rec["mem_penalty"] = float(rec.get("mem_penalty", 0.0)) - memory_penalty
                rec["s"] -= memory_penalty
                # Simple lexical overlap check: drop memories with no query token overlap
                try:
                    text_lower = str(md.get("text") or md.get("information") or "").lower()
                    query_tokens = set()
                    for q in qlist:
                        query_tokens.update(_split_ident(q))
                    has_overlap = any(t in text_lower for t in query_tokens if len(t) >= 3)
                    if not has_overlap:
                        # Strong penalty for irrelevant memories
                        rec["mem_penalty"] -= 0.5
                        rec["s"] -= 0.5
                except Exception:
                    pass

    _dt("boost_loop")

    if timestamps and RECENCY_WEIGHT > 0.0:
        tmin, tmax = min(timestamps), max(timestamps)
        span = max(1, tmax - tmin)
        for rec in score_map.values():
            md = (rec["pt"].payload or {}).get("metadata") or {}
            ts = md.get("last_modified_at") or md.get("ingested_at")
            if isinstance(ts, int):
                norm = (ts - tmin) / span
                rec_comp = RECENCY_WEIGHT * norm
                rec["rec"] += rec_comp
                rec["s"] += rec_comp

    # === Large codebase score normalization ===
    # Spread compressed score distributions for better discrimination
    _normalize_scores(score_map, _coll_size)

    def _tie_key(m: Dict[str, Any]):
        md = (m["pt"].payload or {}).get("metadata") or {}
        sp = str(md.get("symbol_path") or md.get("symbol") or "")
        path = str(md.get("path") or "")
        start_line = int(md.get("start_line") or 0)
        return (-float(m["s"]), len(sp), path, start_line)

    _dt("scoring")
    if os.environ.get("DEBUG_HYBRID_SEARCH"):
        logger.debug(f"score_map has {len(score_map)} items before ranking (coll_size={_coll_size})")
    ranked = sorted(score_map.values(), key=_tie_key)
    if os.environ.get("DEBUG_HYBRID_SEARCH"):
        logger.debug(f"ranked has {len(ranked)} items after sorting")

    # Lightweight keyword bump: prefer spans whose local snippet contains query tokens
    try:
        kb = float(os.environ.get("HYBRID_KEYWORD_BUMP", "0.3") or 0.3)
        kcap = float(os.environ.get("HYBRID_KEYWORD_CAP", "0.6") or 0.6)
    except Exception:
        kb, kcap = 0.3, 0.6
    # Build lowercase keyword set from queries (simple split, keep >=3 chars + special tokens)
    kw: set[str] = set()
    for q in qlist:
        ql = (q or "").lower()
        for tok in re.findall(r"[a-zA-Z0-9_\-]+", ql):
            t = tok.strip()
            if len(t) >= 3:
                kw.add(t)

    import io as _io
    from collections import OrderedDict as _FileCacheOD

    # File content cache to avoid re-reading files for each snippet
    # Controlled by HYBRID_SNIPPET_DISK_READ env var (default ON for backwards compatibility)
    # Uses OrderedDict for LRU eviction to bound memory usage
    # Size limits: max files, max bytes per file, max total cache bytes
    _FILE_LINES_CACHE_MAX = int(os.environ.get("HYBRID_FILE_CACHE_MAX", "100") or 100)
    _FILE_LINES_MAX_FILE_SIZE = int(os.environ.get("HYBRID_FILE_CACHE_MAX_FILE_KB", "256") or 256) * 1024
    _FILE_LINES_MAX_TOTAL_SIZE = int(os.environ.get("HYBRID_FILE_CACHE_MAX_TOTAL_MB", "16") or 16) * 1024 * 1024
    _file_lines_cache: _FileCacheOD[str, List[str]] = _FileCacheOD()
    _file_lines_cache_sizes: dict[str, int] = {}  # Track size of each cached file
    _file_lines_cache_total_size = 0
    _file_lines_cache_lock = threading.Lock()
    _snippet_disk_reads = os.environ.get("HYBRID_SNIPPET_DISK_READ", "1").strip().lower() not in {
        "0", "false", "no", "off"
    }

    def _get_file_lines(path: str) -> List[str]:
        """Get file lines with LRU caching to avoid repeated disk reads.

        Cache key is normalized to realpath to prevent duplicate entries
        for the same file accessed via different path forms.

        Size limits:
        - Files > HYBRID_FILE_CACHE_MAX_FILE_KB (default 256KB) are not cached
        - Total cache size capped at HYBRID_FILE_CACHE_MAX_TOTAL_MB (default 16MB)
        """
        nonlocal _file_lines_cache_total_size

        # Normalize path for consistent cache keys
        p = path
        if not os.path.isabs(p):
            p = os.path.join("/work", p)
        try:
            cache_key = os.path.realpath(p)
        except Exception:
            cache_key = p  # Fallback if realpath fails

        with _file_lines_cache_lock:
            if cache_key in _file_lines_cache:
                # Move to end for LRU ordering
                _file_lines_cache.move_to_end(cache_key)
                return _file_lines_cache[cache_key]

        if not _snippet_disk_reads:
            return []  # Disk reads disabled; caller should handle empty gracefully

        try:
            if cache_key == "/work" or cache_key.startswith("/work/"):
                # Check file size before reading to avoid caching large files
                try:
                    file_size = os.path.getsize(cache_key)
                except OSError:
                    file_size = 0

                with open(cache_key, "r", encoding="utf-8", errors="ignore") as f:
                    lines = f.readlines()

                # Skip caching for large files or if cache is disabled/too small
                if (file_size > _FILE_LINES_MAX_FILE_SIZE
                        or _FILE_LINES_MAX_TOTAL_SIZE <= 0
                        or file_size > _FILE_LINES_MAX_TOTAL_SIZE):
                    return lines

                # LRU eviction: remove oldest entries when at capacity (count or size)
                with _file_lines_cache_lock:
                    # Evict until we have room for this file
                    while (_file_lines_cache_total_size + file_size > _FILE_LINES_MAX_TOTAL_SIZE
                           or len(_file_lines_cache) >= _FILE_LINES_CACHE_MAX):
                        if not _file_lines_cache:
                            break
                        try:
                            evicted_key, _ = _file_lines_cache.popitem(last=False)
                            evicted_size = _file_lines_cache_sizes.pop(evicted_key, 0)
                            _file_lines_cache_total_size -= evicted_size
                        except KeyError:
                            break

                    _file_lines_cache[cache_key] = lines
                    _file_lines_cache_sizes[cache_key] = file_size
                    _file_lines_cache_total_size += file_size
                    _file_lines_cache.move_to_end(cache_key)
                return lines
        except Exception:
            logging.debug("Failed to read file for snippet lines: %s", path, exc_info=True)
        return []

    def _snippet_contains(md: dict) -> int:
        # returns number of keyword hits found in a small local snippet
        try:
            path = str(md.get("path") or "")
            sline = int(md.get("start_line") or 0)
            eline = int(md.get("end_line") or 0)
            txt = (md.get("text") or md.get("code") or "")
            if not txt and path and sline:
                lines = _get_file_lines(path)
                if lines:
                    si = max(1, sline - 3)
                    ei = min(len(lines), max(sline, eline) + 3)
                    txt = "".join(lines[si-1:ei])
            lt = (txt or "").lower()
            if not lt:
                return 0
            hits = 0
            for t in kw:
                if t and t in lt:
                    hits += 1
            return hits
        except Exception:
            return 0

    def _snippet_comment_ratio(md: dict) -> float:
        # Estimate fraction of non-blank lines that are comments (language-agnostic heuristics)
        try:
            path = str(md.get("path") or "")
            sline = int(md.get("start_line") or 0)
            eline = int(md.get("end_line") or 0)
            txt = (md.get("text") or md.get("code") or "")
            if not txt and path and sline:
                lines = _get_file_lines(path)
                if lines:
                    si = max(1, sline - 3)
                    ei = min(len(lines), max(sline, eline) + 3)
                    txt = "".join(lines[si-1:ei])
            if not txt:
                return 0.0
            total = 0
            comment = 0
            in_block = False
            for raw in txt.splitlines():
                line = raw.strip()
                if not line:
                    continue
                total += 1
                # HTML/XML comments
                if line.startswith("<!--"):
                    in_block = True
                    comment += 1
                    continue
                if in_block:
                    comment += 1
                    if "-->" in line:
                        in_block = False
                    continue
                # C/JS block comments
                if line.startswith("/*"):
                    in_block = True
                    comment += 1
                    continue
                if in_block:
                    comment += 1
                    if "*/" in line:
                        in_block = False
                    continue
                # Single-line comments for many languages
                if line.startswith("//") or line.startswith("#"):
                    comment += 1
                    continue
                # Python docstring-like lines (treat as comment-ish for ranking)
                if line.startswith("\"\"\"") or line.startswith("'''"):
                    comment += 1
                    continue
            if total == 0:
                return 0.0
            return comment / float(total)
        except Exception:
            return 0.0

    # Apply bump to top-N ranked (limited for speed)
    topN = min(len(ranked), 200)
    for i in range(topN):
        m = ranked[i]
        md = (m["pt"].payload or {}).get("metadata") or {}
        hits = _snippet_contains(md)
        if hits > 0 and kb > 0.0:
            bump = min(kcap, kb * float(hits))
            m["s"] += bump
        # Apply comment-heavy penalty to de-emphasize comments/doc blocks
        try:
            if COMMENT_PENALTY > 0.0:
                ratio = _snippet_comment_ratio(md)
                thr = float(COMMENT_RATIO_THRESHOLD)
                if ratio >= thr:
                    scale = (ratio - thr) / max(1e-6, 1.0 - thr)
                    pen = min(float(COMMENT_PENALTY), float(COMMENT_PENALTY) * max(0.0, scale))
                    if pen > 0:
                        m["cmt"] = float(m.get("cmt", 0.0)) - pen
                        m["s"] -= pen
        except Exception:
            pass

    # Re-sort after bump
    ranked = sorted(ranked, key=_tie_key)

    # Cluster by path adjacency
    clusters: Dict[str, List[Dict[str, Any]]] = {}
    for m in ranked:
        md = (m["pt"].payload or {}).get("metadata") or {}
        path = str(md.get("path") or "")
        start_line = int(md.get("start_line") or 0)
        end_line = int(md.get("end_line") or 0)
        lst = clusters.setdefault(path, [])
        merged_flag = False
        for c in lst:
            if (
                start_line <= c["end"] + CLUSTER_LINES
                and end_line >= c["start"] - CLUSTER_LINES
            ):
                if float(m["s"]) > float(c["m"]["s"]):
                    c["m"] = m
                c["start"] = min(c["start"], start_line)
                c["end"] = max(c["end"], end_line)
                merged_flag = True
                break
        if not merged_flag:
            lst.append({"start": start_line, "end": end_line, "m": m})

    ranked = sorted([c["m"] for lst in clusters.values() for c in lst], key=_tie_key)
    if os.environ.get("DEBUG_HYBRID_SEARCH"):
        logger.debug(f"ranked has {len(ranked)} items after clustering")


    # Optional MMR diversification (default ON; preserves top-1)
    if _env_truthy(os.environ.get("HYBRID_MMR"), True):
        try:
            _mmr_k = min(len(ranked), max(20, int(os.environ.get("MMR_K", str((limit or 10) * 3)) or 30)))
        except Exception:
            _mmr_k = min(len(ranked), max(20, (limit or 10) * 3))
        try:
            _mmr_lambda = float(os.environ.get("MMR_LAMBDA", "0.7") or 0.7)
        except Exception:
            _mmr_lambda = 0.7
        if (limit or 0) >= 10 or (not per_path) or (per_path <= 0):
            ranked = _mmr_diversify(ranked, k=_mmr_k, lambda_=_mmr_lambda)
            _dt("mmr_diversify")

    # Client-side filters and per-path diversification
    import re as _re, fnmatch as _fnm

    case_sensitive = str(eff_case or "").lower() == "sensitive"

    def _match_glob(pat: str, path: str) -> bool:
        if not pat:
            return True
        if case_sensitive:
            return _fnm.fnmatchcase(path, pat)
        return _fnm.fnmatchcase(path.lower(), pat.lower())

    if eff_not or eff_path_regex or eff_ext or eff_path_globs or eff_not_globs:

        def _pass_filters(m: Dict[str, Any]) -> bool:
            md = (m["pt"].payload or {}).get("metadata") or {}
            path = str(md.get("path") or "")
            rel = path[6:] if path.startswith("/work/") else path
            pp = str(md.get("path_prefix") or "")
            p_for_sub = path if case_sensitive else path.lower()
            pp_for_sub = pp if case_sensitive else pp.lower()
            if eff_not:
                nn = eff_not if case_sensitive else eff_not.lower()
                if nn in p_for_sub or nn in pp_for_sub:
                    return False
            if eff_not_globs_norm and any(_match_glob(g, path) or _match_glob(g, rel) for g in eff_not_globs_norm):
                return False
            if eff_ext:
                ex = eff_ext.lower().lstrip(".")
                if not path.lower().endswith("." + ex):
                    return False
            if eff_path_regex:
                flags = 0 if case_sensitive else _re.IGNORECASE
                try:
                    if not _re.search(eff_path_regex, path, flags=flags):
                        return False
                except Exception:
                    pass
            if eff_path_globs_norm and not any(_match_glob(g, path) or _match_glob(g, rel) for g in eff_path_globs_norm):
                return False
            return True

        ranked = [m for m in ranked if _pass_filters(m)]

    # ReFRAG-lite span compaction and budgeting is NOT applied here in run_hybrid_search
    # It's only applied in context_answer where token budgeting is needed for LLM context
    # Removing this to avoid over-filtering search results

    # === Entity-level deduplication (anti-collapse) ===
    # Deduplicate by entity (code_id/doc_id or path+symbol) BEFORE final limit slice.
    # This prevents duplicate chunks from the same entity from wasting top-k slots.
    # Critical for chunked indexing where multiple points per entity can saturate
    # rankings with duplicates as corpus grows (Bruch et al. TOIS 2023).
    _entity_dedup_enabled = os.environ.get("HYBRID_ENTITY_DEDUP", "1").strip().lower() not in {
        "0", "false", "no", "off"
    }

    def _extract_entity_key(m: Dict[str, Any]) -> str:
        """Extract entity identifier for deduplication.

        Priority:
        1. code_id (CoSQA/benchmarks)
        2. doc_id or _id (CoIR/benchmarks)
        3. path + symbol + kind (regular code - symbol-level entity)
        4. path (file-level fallback)
        """
        try:
            pt = m.get("pt")
            if pt and pt.payload:
                payload = pt.payload
                # Check for benchmark entity IDs first
                code_id = payload.get("code_id")
                if code_id:
                    return f"code_id:{code_id}"
                doc_id = payload.get("doc_id") or payload.get("_id")
                if doc_id:
                    return f"doc_id:{doc_id}"
                # Fall back to path+symbol for regular code
                md = payload.get("metadata") or {}
                path = str(md.get("path") or "")
                symbol = str(md.get("symbol") or "")
                kind = str(md.get("kind") or "")
                if path and symbol and kind:
                    # Symbol-level entity (preferred for code)
                    return f"entity:{path}:{kind}:{symbol}"
                elif path:
                    # File-level entity (fallback when no symbol)
                    return f"file:{path}"
        except Exception:
            pass
        # Fallback: use point ID (no deduplication)
        try:
            pt_id = m.get("pt")
            if pt_id and hasattr(pt_id, "id"):
                return f"point:{pt_id.id}"
        except Exception:
            pass
        return f"point:{id(m)}"

    if _entity_dedup_enabled:
        # Deduplicate ranked list, keeping first (highest-scoring) occurrence of each entity.
        # Implements entity-level fusion (max-score aggregation) as recommended by
        # Bruch et al. (TOIS 2023) for hybrid retrieval with chunked corpora.
        # Early-stop once we have enough unique entities (more efficient than processing all).
        seen_entities: set[str] = set()
        deduped_ranked: List[Dict[str, Any]] = []
        # Calculate how many unique entities we need (account for per_path limiting later)
        # If per_path is set, we might need more candidates; otherwise stop at limit * safety_factor
        target_entities = limit * 3 if (per_path and per_path > 0) else limit * 2
        chunks_scanned = 0

        for m in ranked:
            chunks_scanned += 1
            entity_key = _extract_entity_key(m)
            if entity_key not in seen_entities:
                seen_entities.add(entity_key)
                deduped_ranked.append(m)
                # Early stop once we have enough unique entities
                if len(deduped_ranked) >= target_entities:
                    break

        if os.environ.get("DEBUG_HYBRID_SEARCH"):
            logger.debug(
                f"Entity deduplication: scanned {chunks_scanned}/{len(ranked)} chunks -> "
                f"{len(deduped_ranked)} unique entities (removed {chunks_scanned - len(deduped_ranked)} duplicates)"
            )
        ranked = deduped_ranked

    if per_path and per_path > 0:
        counts: Dict[str, int] = {}
        merged: List[Dict[str, Any]] = []
        if os.environ.get("DEBUG_HYBRID_SEARCH"):
            logger.debug(f"Applying per_path={per_path} limiting to {len(ranked)} ranked results")
        for m in ranked:
            md = (m["pt"].payload or {}).get("metadata") or {}
            path = str(md.get("path", ""))
            c = counts.get(path, 0)
            if c < per_path:
                merged.append(m)
                counts[path] = c + 1
            if len(merged) >= limit:
                break
        if os.environ.get("DEBUG_HYBRID_SEARCH"):
            logger.debug(f"After per_path limiting: {len(merged)} results from {len(counts)} unique paths")
    else:
        merged = ranked[:limit]

    # Emit structured items
    # Build directory → paths map for related hints (same dir siblings)
    dir_to_paths: Dict[str, set] = {}
    try:
        for _m in merged:
            _md = (_m["pt"].payload or {}).get("metadata") or {}
            _pp = str(_md.get("path_prefix") or "")
            _p = str(_md.get("path") or "")
            if _pp and _p:
                dir_to_paths.setdefault(_pp, set()).add(_p)
    except Exception:
        dir_to_paths = {}
    # Precompute known paths for quick membership checks
    all_paths: set = set()
    try:
        for _s in dir_to_paths.values():
            all_paths |= set(_s)
    except Exception:
        all_paths = set()

    # Build path -> host_path map so we can emit related_paths in host space
    # when PATH_EMIT_MODE prefers host paths. This keeps human-facing paths
    # consistent while still preserving container paths for backend use.
    host_map: Dict[str, str] = {}
    try:
        for _m in merged:
            _md = (_m["pt"].payload or {}).get("metadata") or {}
            _p = str(_md.get("path") or "").strip()
            _h = str(_md.get("host_path") or "").strip()
            if _p and _h:
                host_map[_p] = _h
    except Exception:
        host_map = {}

    items: List[Dict[str, Any]] = []
    if not merged:
        if _USE_CACHE and cache_key is not None:
            if UNIFIED_CACHE_AVAILABLE:
                # Use unified caching system
                _RESULTS_CACHE.set(cache_key, items)
                if os.environ.get("DEBUG_HYBRID_SEARCH"):
                    logger.debug("cache store for hybrid results")
            else:
                # Fallback to original caching system
                with _RESULTS_LOCK:
                    _RESULTS_CACHE[cache_key] = items
                    if os.environ.get("DEBUG_HYBRID_SEARCH"):
                        logger.debug("cache store for hybrid results")
                    while len(_RESULTS_CACHE) > MAX_RESULTS_CACHE:
                        _RESULTS_CACHE.popitem(last=False)
        return items

    for m in merged:
        md = (m["pt"].payload or {}).get("metadata") or {}
        # Prefer merged bounds if present
        start_line = m.get("_merged_start") or md.get("start_line")
        end_line = m.get("_merged_end") or md.get("end_line")
        # Store both fusion_score (from hybrid) and rerank_score (if available) separately
        fusion_score = float(m.get("s", 0.0))
        rerank_score = m.get("rerank_score")  # None if not reranked

        comp = {
            "dense_rrf": round(float(m.get("d", 0.0)), 4),
            "lexical": round(float(m.get("lx", 0.0)), 4),
            "symbol_substr": round(float(m.get("sym_sub", 0.0)), 4),
            "symbol_exact": round(float(m.get("sym_eq", 0.0)), 4),
            "fname_boost": round(float(m.get("fname", 0.0)), 4),
            "core_boost": round(float(m.get("core", 0.0)), 4),
            "vendor_penalty": round(float(m.get("vendor", 0.0)), 4),
            "lang_boost": round(float(m.get("langb", 0.0)), 4),
            "recency": round(float(m.get("rec", 0.0)), 4),
            "test_penalty": round(float(m.get("test", 0.0)), 4),
            # new components
            "config_penalty": round(float(m.get("cfg", 0.0)), 4),
            "impl_boost": round(float(m.get("impl", 0.0)), 4),
            "doc_penalty": round(float(m.get("doc", 0.0)), 4),
        }

        # Add reranker info to components if present
        if rerank_score is not None:
            comp["rerank"] = round(float(rerank_score), 4)
        # Build "why" explanation only if enabled (reduces tokens by default)
        why = None
        if INCLUDE_WHY:
            why = []
            if comp["dense_rrf"]:
                why.append(f"dense_rrf:{comp['dense_rrf']}")
            for k in ("lexical", "symbol_substr", "symbol_exact", "fname_boost", "core_boost", "lang_boost", "impl_boost"):
                if comp[k]:
                    why.append(f"{k}:{comp[k]}")
            if comp["vendor_penalty"]:
                why.append(f"vendor_penalty:{comp['vendor_penalty']}")
            for k in ("test_penalty", "config_penalty", "doc_penalty"):
                if comp.get(k):
                    why.append(f"{k}:{comp[k]}")
            if comp["recency"]:
                why.append(f"recency:{comp['recency']}")
        # Related hints
        _imports = md.get("imports") or []
        _calls = md.get("calls") or []
        _symp = md.get("symbol_path") or md.get("symbol") or ""
        _pp = str(md.get("path_prefix") or "")
        _path = str(md.get("path") or "")
        _related_set = set()
        # Same-dir siblings
        try:
            if _pp in dir_to_paths:
                for p in dir_to_paths[_pp]:
                    if p != _path:
                        _related_set.add(p)
        except Exception:
            pass
        # Import-based hints: resolve relative/quoted path-like imports
        try:
            import re as _re, posixpath as _ppath

            def _pathlike_segments(s: str) -> list[str]:
                s = str(s or "")
                segs = []
                # quoted segments first
                for mmm in _re.findall(r"[\"']([^\"']+)[\"']", s):
                    if "/" in mmm or mmm.startswith("."):
                        segs.append(mmm)
                # fall back to whitespace tokens containing '/' or starting with '.'
                for tok in str(s).replace(",", " ").split():
                    if ("/" in tok) or tok.startswith("."):
                        segs.append(tok)
                return segs

            def _resolve(seg: str) -> list[str]:
                try:
                    seg = seg.strip()
                    # base dir from path_prefix
                    base = _pp or ""
                    candidates = []
                    # choose join rule
                    if seg.startswith("./") or seg.startswith("../") or "/" in seg:
                        j = _ppath.normpath(_ppath.join(base, seg)) if not seg.startswith("/") else _ppath.normpath(seg)
                        candidates.append(j)
                        # add extensions if last segment lacks a dot
                        last = j.split("/")[-1]
                        if "." not in last:
                            for ext in [".py", ".js", ".ts", ".tsx", ".jsx", ".mjs", ".cjs"]:
                                candidates.append(j + ext)
                    out = set()
                    for c in candidates:
                        if c in all_paths:
                            out.add(c)
                        if c.startswith("/") and c.lstrip("/") in all_paths:
                            out.add(c.lstrip("/"))
                        if c.startswith("/work/") and c[len("/work/"):] in all_paths:
                            out.add(c[len("/work/"):])
                    return list(out)
                except Exception:
                    return []

            for imp in (_imports or []):
                for seg in _pathlike_segments(imp):
                    for cand in _resolve(seg):
                        if cand != _path:
                            _related_set.add(cand)
        except Exception:
            pass

        _related = sorted(_related_set)[:10]
        # Align related_paths with PATH_EMIT_MODE when possible: in host/auto
        # modes, prefer host paths when we have a mapping; in container mode,
        # keep container/path-space values as-is.
        _related_out = _related
        try:
            _mode_related = str(os.environ.get("PATH_EMIT_MODE", "auto")).strip().lower()
        except Exception:
            _mode_related = "auto"
        if _mode_related in {"host", "auto"}:
            try:
                _mapped: List[str] = []
                for rp in _related:
                    _mapped.append(host_map.get(rp, rp))
                _related_out = _mapped
            except Exception:
                _related_out = _related
        # Best-effort snippet text directly from payload for downstream LLM stitching
        _payload = (m["pt"].payload or {}) if m.get("pt") is not None else {}
        _metadata = _payload.get("metadata", {}) or {}
        _text = (
            _payload.get("code") or
            _metadata.get("code") or
            _payload.get("text") or
            _metadata.get("text") or
            ""
        )
        # Carry through pseudo/tags so downstream consumers (e.g., repo_search reranker)
        # can incorporate index-time GLM/llm labels into their own scoring or display.
        _pseudo = _payload.get("pseudo")
        if _pseudo is None:
            _pseudo = _metadata.get("pseudo")
        _tags = _payload.get("tags")
        if _tags is None:
            _tags = _metadata.get("tags")
        # Skip memory-like points without a real file path
        if not _path or not _path.strip():
            if os.environ.get("DEBUG_HYBRID_FILTER"):
                logger.debug(f"Filtered out item with empty path: {_metadata}")
            continue

        # Emit path: prefer original host path when available; also include container path
        _emit_path = _path
        _host = ""
        _cont = ""
        try:
            _host = str(_metadata.get("host_path") or "").strip()
            _cont = str(_metadata.get("container_path") or "").strip()
            _repo = str(_metadata.get("repo") or "").strip()
            _pp = str(_metadata.get("path_prefix") or "").strip()
            _mode = str(os.environ.get("PATH_EMIT_MODE", "auto")).strip().lower()

            if _mode == "host" and _host:
                _emit_path = _host
            elif _mode == "container" and _cont:
                _emit_path = _cont
            else:
                # Auto mode: prefer host when available, else container; then fallback normalization
                if _host:
                    _emit_path = _host
                elif _cont:
                    _emit_path = _cont
                else:
                    # Auto/compat fallback: normalize to container form if repo+prefix known; else map cwd to /work
                    if _repo and _pp and isinstance(_emit_path, str):
                        _pp_norm = _pp.rstrip("/") + "/"
                        if _emit_path.startswith(_pp_norm):
                            _rel = _emit_path[len(_pp_norm):]
                            if _rel:
                                _emit_path = f"/work/{_repo}/" + _rel.lstrip("/")
                    if isinstance(_emit_path, str):
                        _cwd = os.getcwd().rstrip("/") + "/"
                        if _emit_path.startswith(_cwd):
                            _rel = _emit_path[len(_cwd):]
                            if _rel:
                                _emit_path = "/work/" + _rel
        except Exception:
            pass

        # Extract payload for benchmark consumers (code_id, _id, etc.)
        _payload_out = None
        try:
            _pt = m.get("pt")
            if _pt is not None and _pt.payload:
                # Return full payload (excluding large fields already in result)
                _payload_out = dict(_pt.payload)
                # Remove 'metadata' if present - it's already unpacked into result fields
                _payload_out.pop("metadata", None)
        except Exception:
            pass

        item = {
            "score": round(float(m["s"]), 4),
            "raw_score": float(m["s"]),  # expose raw fused score for downstream budgeter
            "fusion_score": round(fusion_score, 4),  # Always store fusion score
            "rerank_score": round(float(rerank_score), 4) if rerank_score is not None else None,  # Store rerank separately
            "payload": _payload_out,  # Raw payload for benchmarks (code_id, _id, etc.)
            "path": _emit_path,
            "host_path": _host,
            "container_path": _cont,
            "symbol": _symp,
            "start_line": start_line,
            "end_line": end_line,
            "components": comp,
            "relations": {"imports": _imports, "calls": _calls, "symbol_path": _symp},
            "related_paths": _related_out,
            "span_budgeted": bool(m.get("_merged_start") is not None),
            "budget_tokens_used": m.get("_budget_tokens"),
            "text": _text,
            "pseudo": _pseudo,
            "tags": _tags,
        }
        if why is not None:
            item["why"] = why
        items.append(item)
    if _USE_CACHE and cache_key is not None:
        if UNIFIED_CACHE_AVAILABLE:
            _RESULTS_CACHE.set(cache_key, items)
            # Mirror into local fallback dict for deterministic hits in tests
            try:
                with _RESULTS_LOCK:
                    _RESULTS_CACHE_OD[cache_key] = items
                    while len(_RESULTS_CACHE_OD) > MAX_RESULTS_CACHE:
                        # pop oldest inserted (like LRU/FIFO)
                        try:
                            _RESULTS_CACHE_OD.popitem(last=False)
                        except Exception:
                            break
            except Exception:
                pass
            if os.environ.get("DEBUG_HYBRID_SEARCH"):
                logger.debug("cache store for hybrid results")
        else:
            with _RESULTS_LOCK:
                _RESULTS_CACHE[cache_key] = items
                if os.environ.get("DEBUG_HYBRID_SEARCH"):
                    logger.debug("cache store for hybrid results")
                while len(_RESULTS_CACHE) > MAX_RESULTS_CACHE:
                    _RESULTS_CACHE.popitem(last=False)
    _dt("total")
    return items


def main():
    """CLI entrypoint - delegates to run_hybrid_search and formats output."""
    ap = argparse.ArgumentParser(description="Hybrid search: dense + lexical RRF")
    ap.add_argument("--query", "-q", action="append", required=True, help="Query strings")
    ap.add_argument("--language", type=str, default=None)
    ap.add_argument("--under", type=str, default=None)
    ap.add_argument("--kind", type=str, default=None)
    ap.add_argument("--symbol", type=str, default=None)
    ap.add_argument("--expand", dest="expand", action="store_true",
                    default=_env_truthy(os.environ.get("HYBRID_EXPAND"), False))
    ap.add_argument("--no-expand", dest="expand", action="store_false")
    ap.add_argument("--per-path", type=int, default=int(os.environ.get("HYBRID_PER_PATH", "1") or 1))
    _per_query_env = os.environ.get("HYBRID_PER_QUERY")
    _per_query_default = int(_per_query_env) if _per_query_env and _per_query_env.strip().isdigit() else None
    ap.add_argument("--per-query", type=int, default=_per_query_default,
                    help="Candidate retrieval per query (default: adaptive based on limit/collection size). "
                         "Also settable via HYBRID_PER_QUERY env var.")
    ap.add_argument("--limit", type=int, default=10)
    ap.add_argument("--json", dest="json_out", action="store_true")
    ap.add_argument("--quiet", dest="quiet", action="store_true")
    ap.add_argument("--ext", type=str, default=None)
    ap.add_argument("--not", dest="not_filter", type=str, default=None)
    ap.add_argument("--collection", type=str, default=None)
    ap.add_argument("--case", type=str, choices=["sensitive", "insensitive"],
                    default=os.environ.get("HYBRID_CASE", "insensitive"))
    ap.add_argument("--path-regex", dest="path_regex", type=str, default=None)
    ap.add_argument("--path-glob", dest="path_glob", type=str, default=None)
    ap.add_argument("--not-glob", dest="not_glob", type=str, default=None)
    ap.add_argument("--repo", type=str, default=None, help="Filter by repo name(s)")
    args = ap.parse_args()

    # Delegate to run_hybrid_search
    results = run_hybrid_search(
        queries=args.query,
        limit=args.limit,
        per_path=args.per_path,
        language=args.language,
        under=args.under,
        kind=args.kind,
        symbol=args.symbol,
        ext=args.ext,
        not_filter=args.not_filter,
        case=args.case,
        path_regex=args.path_regex,
        path_glob=args.path_glob,
        not_glob=args.not_glob,
        expand=args.expand,
        collection=args.collection,
        repo=args.repo,
        per_query=args.per_query,
    )

    # Handle empty results
    if not results:
        if args.quiet:
            sys.exit(1)
        print("No results found.", file=sys.stderr)
        return

    # Output results
    for item in results:
        if args.json_out:
            print(json.dumps(item))
        else:
            print(f"{item.get('score', 0):.3f}\t{item.get('path')}\t{item.get('symbol', '')}\t{item.get('start_line')}-{item.get('end_line')}")


if __name__ == "__main__":
    main()
